## **TL;DR**

- MDP는 강화 학습의 핵심 개념으로, 상태와 행동 간의 확률적 전이를 모델링하는 수학적 프레임워크임.
- 가치 함수와 벨만 방정식을 이용해 최적 정책을 찾을 수 있음.
- 동적 프로그래밍, 모델 기반 학습, 모델 프리 강화 학습 등을 통해 해결 가능함.

## **MDP의 기본 개념**

- 상태(\( S \)): 환경이 가질 수 있는 상태들의 집합.
- 행동(\( A \)): 에이전트가 선택할 수 있는 행동들의 집합.
- 전이 확률(\( P(s' | s, a) \)): 현재 상태에서 특정 행동을 수행했을 때 다음 상태로 전이될 확률.
- 보상(\( R(s, a) \)): 특정 상태에서 행동을 수행했을 때 얻는 보상.
- 할인율(\( \gamma \)): 미래 보상의 현재 가치에 대한 할인 계수.

## **마코프 속성 (Markov Property)**

- 현재 상태만 알면 미래 상태를 예측할 수 있음.
- 과거 상태들은 고려하지 않아도 됨.
- 확률적 전이 함수:

  \[
  P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ...)
  \]

## **정책과 가치 함수**

### **정책(Policy, \( \pi \))**

- 정책은 상태에서 행동을 결정하는 규칙.
- 확정적 정책: \( \pi(s) = a \)
- 확률적 정책: \( \pi(a | s) \)

### **가치 함수(Value Function)**

- 상태 가치 함수(\( V^\pi(s) \)): 특정 정책을 따를 때 해당 상태에서 받을 기대 보상.
- 행동 가치 함수(\( Q^\pi(s, a) \)): 특정 행동을 취했을 때 받을 기대 보상.

\[
V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(S_t, A_t) \mid S_0 = s, \pi \right]
\]

\[
Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(S_t, A_t) \mid S_0 = s, A_0 = a, \pi \right]
\]

## **벨만 방정식과 최적 정책**

### **벨만 기대 방정식**

- 상태 가치 함수:

  \[
  V^\pi(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) [ R(s, a) + \gamma V^\pi(s') ]
  \]

### **벨만 최적 방정식**

- 최적 상태 가치 함수:

  \[
  V^*(s) = \max_{a} \sum_{s'} P(s' | s, a) [ R(s, a) + \gamma V^*(s') ]
  \]
- 최적 행동 가치 함수:

  \[
  Q^*(s, a) = \sum_{s'} P(s' | s, a) [ R(s, a) + \gamma \max_{a'} Q^*(s', a') ]
  \]

## **MDP 해결 방법**

### **동적 프로그래밍(Dynamic Programming, DP)**

- 환경의 전이 확률을 알고 있을 때, 벨만 방정식을 반복적으로 계산하여 최적 정책을 찾음.
- 정책 반복(Policy Iteration)과 가치 반복(Value Iteration) 방법이 있음.

### **모델 기반 학습(Model-Based Methods)**

- 환경의 전이 확률이 주어졌을 때 이를 이용해 최적 정책을 계산.

### **모델 프리 강화 학습(Model-Free RL)**

- 환경의 전이 확률을 모를 때 데이터를 통해 학습.
- 대표적인 알고리즘:
    - **Q-Learning**: 행동 가치 함수 \( Q(s, a) \)를 업데이트하여 최적 정책 학습.
    - **SARSA**: 다음 행동도 정책을 통해 결정하여 학습.

## **MDP 활용 예시**

### **강화 학습 (Reinforcement Learning)**

- 자율주행, 게임 AI, 로봇 제어 등에서 최적 행동을 찾는 데 사용됨.

### **자연어 처리 (NLP)**

- 챗봇, 기계 번역에서 대화 전략을 최적화하는 데 적용됨.

### **추천 시스템**

- 사용자 행동을 예측하고 맞춤형 추천을 생성하는 데 활용됨.

## **결론**

- MDP는 강화 학습의 수학적 기반이며, 최적 정책을 찾기 위한 핵심 개념임.
- 가치 함수와 벨만 방정식을 활용해 최적 정책을 계산할 수 있음.
- 동적 프로그래밍, 모델 기반 학습, 모델 프리 학습을 통해 다양한 방법으로 해결 가능함.
