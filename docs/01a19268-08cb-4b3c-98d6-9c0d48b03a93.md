---
publish: true
id: TSK-2418
uuid: 01a19268-08cb-4b3c-98d6-9c0d48b03a93
link: https://chatgpt.com/c/68084cf7-8e3c-8009-812a-e218800f9748
created: 2025-05-25
---

# LLM 생성 수학 퀴즈의 교육적 품질 평가 방법론

## 개요 (Introduction)

K-12 수준의 수학 문제를 대규모 언어 모델(LLM)이 생성하는 사례가 늘면서, 이러한 자동 생성 퀴즈의 **교육적 품질**을 평가하는 것이 중요해졌습니다. 특히 이번 보고서는 **선택형 객관식** 수학 퀴즈를 중심으로, 문제를 작은 단위로 분해한 _퀴즈 체인(quiz chain)_ 형식이나 원문제를 변형한 **유사 문제 생성** 상황에서 해당 퀴즈의 품질을 **정량적으로** 평가할 수 있는 지표와 방법을 조사합니다. 아래에서는 LLM이 생성한 수학 퀴즈의 품질을 나타내는 주요 지표들과 그 정의를 정리하고, 각 지표를 실제로 측정하거나 구현하는 방법을 제시합니다. 또한 **오답 선택지(distractor)**의 품질 평가, 문제 분해형 퀴즈의 학습 효과 측정, 실데이터 부재 시 **모의 학습자 시뮬레이션**을 통한 간접 평가 방법, 그리고 Khan Academy, ASSISTments, EdNet 등 실제 시스템이나 연구 사례에서 사용된 평가 방식을 함께 다룹니다.

## LLM 생성 퀴즈 평가 지표 목록 및 정의

LLM이 만든 수학 퀴즈의 품질을 평가하기 위한 대표적인 **정량적 지표**들은 다음과 같습니다. 각 지표의 정의와 중요성을 표 1에 요약합니다.

<table> <thead> <tr><th>평가 지표</th><th>정의 및 설명</th></tr> </thead> <tbody> <tr> <td><b>난이도 (Difficulty)</b></td> <td>문항이 학습자에게 얼마나 어려운지 수준을 나타냄. 보통 정답률(정답자 비율)로 표현되며, 낮은 정답률일수록 어려운 문항을 의미함.</td> </tr> <tr> <td><b>난이도 균형<br>(Difficulty Balance)</b></td> <td>퀴즈 또는 문항 세트 내 난이도의 분포가 적절한지 여부. 너무 쉬운 문제나 너무 어려운 문제로 치우치지 않고, 다양한 난이도가 균형 있게 포함되어 있는지를 평가.</td> </tr> <tr> <td><b>오답 선택지 품질<br>(Distractor Quality)</b></td> <td>객관식 문제의 오답 보기가 얼마나 잘 설계되었는지. 오답이 지나치게 쉽거나 터무니없지 않고, 그럴듯해서 학습자가 틀릴 경우 선택할 법한지 (즉, <i>매력적인 오답</i>)를 평가함.</td> </tr> <tr> <td><b>정답 추측 용이성<br>(Guessability)</b></td> <td>문항에 지식이 없더라도 정답을 알아맞힐 수 있는 단서가 존재하는지 여부. 예를 들어 정답만 유난히 길거나 구체적이라서 눈에 띄는 경우 등 <i>문항 편향</i>이 있다면 정답을 쉽게 추측 가능함.</td> </tr> <tr> <td><b>개념 포괄성<br>(Concept Coverage)</b></td> <td>퀴즈가 목표로 하는 개념이나 지식을 얼마나 포괄적으로 다루는지. 한 문제나 문제 세트가 해당 단원의 핵심 개념들을 골고루 포함하고 있는지를 나타냄.</td> </tr> <tr> <td><b>학습 목표 정렬<br>(Alignment with Objectives)</b></td> <td>문항이 교육과정의 성취기준 또는 학습 목표에 부합하는 정도. 가르치려는 개념/기능과 평가하는 문제가 일치하며, 부적절한 내용이나 범위를 다루지 않는지 평가.</td> </tr> <tr> <td><b>문항 명확성<br>(Clarity)</b></td> <td>문항의 표현이 명확하고 오해의 소지가 없는지. 문장부호, 용어, 지시사항 등이 모호하지 않고 학습자가 문제 이해에 불필요한 어려움을 겪지 않는지를 평가.</td> </tr> <tr> <td><b>정확성<br>(Solution Correctness)</b></td> <td>문항의 정답 및 해설이 실제로 옳고, 문제에 오류나 잘못된 전제가 없는지. LLM 생성 콘텐츠에서는 가끔 오류가 섞일 수 있으므로, 반드시 사실적/수학적 정확성을 확인해야 함.</td> </tr> </tbody> </table>

표 1. LLM 생성 수학 퀴즈 품질 평가 지표 및 정의

위의 지표들은 퀴즈의 다양한 측면을 다각도로 살피기 위한 것입니다. 예컨대 **난이도**와 **난이도 균형**은 문항들이 **학습자 수준에 적합한지**와 세트 전체의 **평균/분산**을 평가하고, **오답 품질**과 **정답 추측 용이성**은 문항의 **평가 역량**과 **공정성**을 점검합니다. **개념 포괄성**과 **목표 정렬**은 **교육과정 상의 적합성**을 의미하며, **명확성**과 **정확성**은 **기초적인 완성도**를 담보합니다.

이러한 지표들은 상호 보완적으로 사용되어, LLM이 생성한 문제가 **학습 효과**와 **평가 도구로서의 신뢰도**를 갖추고 있는지 판단하게 됩니다.

## 지표별 구현 및 측정 방법

각 지표를 실제로 **정량화하거나 평가하는 방법**에는 여러 접근이 존재합니다. LLM을 활용한 자동 평가부터 전문가에 의한 평가, 학습자 데이터 분석까지 다양한 기법을 조합해 사용할 수 있습니다. 아래에서는 지표별 또는 방법론별로 구체적인 측정 방안을 설명합니다.

- **난이도(Difficulty) 측정:** 학습자 **정답률**은 난이도를 직접 나타내는 지표입니다. 만약 실제 학습자 응답 데이터가 있다면, 해당 문항을 맞힌 비율(%)이나 평균 점수를 통해 난이도를 산출할 수 있습니다. 데이터가 없을 경우, **LLM이나 ML 모델을 활용한 난이도 예측**이 가능합니다. 예를 들어, 기출문항에 대한 난이도 정보를 학습한 모델에 새 문항을 투입해 **예상 정답률**을 예측하거나, LLM에게 “이 문제는 쉬움/보통/어려움 중 어떤 난이도로 예상되는가?”를 평가하게 할 수 있습니다. 실제 연구에서도 **문항 난이도 사전 예측**을 시도하고 있으며, BERT 등의 NLP 모델로 문제 텍스트로부터 난이도를 추정하는 접근이 보고되었습니다. 최근에는 LLM을 **가상의 수험생**처럼 활용하여, 여러 서로 다른 QA 모델에 문제를 풀게 한 뒤 맞힌 여부로 난이도를 예측하는 기법도 제안되었습니다. 예컨대 Gao 등은 두 개의 QA 시스템이 모두 맞히면 “easy”, 모두 틀리면 “hard”로 이진 분류하였고, Byrd와 Uto 등은 다수의 QA 시스템 정오 답변을 아이템 반응이론(IRT)에 적용하여 난이도를 추정하였습니다. 이러한 **“LLM을 시험보는 학생처럼 활용”**하는 방법은 별도의 정답 데이터 수집 없이도 난이도를 예측할 수 있다는 장점이 있습니다. 다만, LLM과 실제 학생의 능력 척도가 다를 수 있으므로, Uto 등은 QA 시스템들의 성능을 인간의 능력 척도에 **교정(test linking)**하여 예측 난이도가 인간 기준과 맞도록 조정하는 방법을 제시하기도 했습니다. 이처럼 **데이터 기반 난이도 예측**은 실제 학생 응답 데이터나 시뮬레이션을 통해 구현할 수 있으며, 특히 적응형 학습 시스템에서는 문항 난이도 추정이 **개인화**와 **학습 경로 최적화**에 핵심적으로 활용됩니다.

- **난이도 균형 평가:** 난이도 자체뿐 아니라 **세트 전체의 난이도 분포**도 중요합니다. 이를 정량적으로 평가하려면, 한 퀴즈에 포함된 여러 문항의 예상 난이도를 산출한 뒤, 그 평균과 표준편차 또는 범위를 확인합니다. **난이도 균형**은 적절한 난이도 문항이 골고루 섞여 있는지를 보는 것으로, 예를 들어 평균 난이도는 중간 정도이지만 각 문항이 지나치게 쉽거나 어렵지 않게 분포하는 것이 이상적입니다. 이를 수치화하기 위해 각 문항 난이도의 **분산**이 너무 크지 않은지 확인하거나, 쉬움/보통/어려움 문항의 비율이 교육 설계 의도에 맞게 배치되었는지 점검합니다. 만약 **퀴즈 체인**처럼 문제를 단계별로 분해한 경우, **초기 문항은 쉽고 뒤로 갈수록 어려워지는지** 등 난이도 상승 곡선이 합리적인지도 평가 요소가 될 수 있습니다. 이러한 균형도 LLM이나 규칙 기반으로 진단할 수 있는데, 예를 들어 LLM에게 "각 문항의 난이도를 1~5로 평가하고 고르게 분포하는지 확인"하도록 지시할 수 있습니다. 혹은 **난이도 매핑 테이블**을 두어, 각 문항을 Bloom의 Taxonomy 단계나 학년 기준으로 난이도 분류한 뒤 적절한 분포인지 확인하는 방법도 있습니다.

- **오답 선택지 품질(Distractor Quality) 평가:** 객관식 문제의 **정답 이외의 보기**(오답)의 품질은 문제의 변별력과 공정성에 큰 영향을 줍니다. 오답 품질을 정량화하려면 우선 오답이 **얼마나 많은 학생들을 현혹시키는지**를 살펴봐야 합니다. 실제 데이터가 있다면 각 오답 선택지가 선택된 비율을 분석하여, **너무 적게 선택된 오답**은 없는지 확인할 수 있습니다. 통상적으로 전체 학생 중 **5% 미만**만 선택한 보기들은 _비기능성 오답_(non-functional distractor)으로 간주하며, 이런 오답이 많은 문항은 질이 낮다고 평가합니다. 반대로 하나의 오답이 유독 과도하게 선택됐다면, 그 오답이 **너무 매력적이거나 혼동을 주는 요소**는 없는지 검토해야 합니다. 데이터가 없을 경우, **사람 또는 LLM을 통한 평가**로 오답의 매력도를 판단할 수 있습니다. 예를 들어 **human rubric**을 사용해 전문가들이 각 오답에 1~5점의 _매력도 점수_를 매기거나, “이 오답이 그럴듯한지” 여부를 판단하게 할 수 있습니다. LLM을 활용한다면, “문제와 정답을 참고하여 각 보기마다 오답으로서 타당한지 평가”라는 프롬프트를 줄 수 있습니다. 실제 연구에서도 LLM 기반으로 생성된 MCQ를 평가할 때 **오답의 그럴듯함(plausibility)**을 주요 기준으로 삼았습니다. 한 예에서 LLM이 생성한 데이터 과학 강의 문제를 평가한 결과, 사람 평가자들이 지적한 가장 흔한 결함 중 하나가 “신뢰할 수 없는 오답 선택지 존재(57% 빈도)”였다고 보고됩니다. 이는 곧 절반 이상의 문항에 **적어도 하나의 부적절한 오답**이 있었다는 뜻입니다. 따라서 오답 품질 평가를 위해 다음과 같은 하위 요소를 점검합니다:

    - _오답 현실성/관련성:_ 오답이 문제의 맥락에서 말이 되고, 질문에서 요구하는 개념과 연관이 있어야 합니다. 전혀 문맥에 맞지 않는 오답은 금방 눈치챌 수 있으므로 피해합니다. 예를 들어 “프랑스의 수도는?”라는 질문에 “파리 힐튼”과 같은 오답은 단어는 비슷해 보여도 맥락상 부적절한 **나쁜 오답**입니다.

    - _오답 동질성:_ 정답과 오답들이 **형식과 범주 면에서 유사한지**를 봅니다. 예를 들어 정답이 수학적 값이라면 모든 오답도 비슷한 유형(숫자 값)이어야 하고, 정답이 특정 범위의 수라면 오답들도 그 범위에서 벗어나지 않아야 합니다. 한 문항에서 보기들이 서로 이질적이면 눈에 띄는 보기가 생겨 정답일 확률을 높일 수 있습니다.

    - _오답 그럴듯함(매력도):_ 각 오답이 **피험자를 충분히 속일 수 있을 만큼 그럴듯한지** 평가합니다. 이상적인 오답은 해당 개념을 부분적으로 알고 있거나 흔히 하는 실수를 반영하여, 학습자가 충분한 이해가 없으면 정답 대신 고를법한 선택지입니다. 예컨대 계산 문제에서 흔히 틀리는 계산 오류 결과값을 오답으로 넣으면 매력적인 오답이 됩니다.

    - _비기능성 오답 여부:_ 앞서 언급한 대로, 전혀 선택되지 않을 정도로 엉뚱한 오답은 없는지 점검합니다. 이런 오답이 발견되면 해당 보기들을 **개선하거나 제거**하는 것이 바람직합니다.

    이러한 오답 품질 요소들은 전문가의 눈으로 평가할 수도 있지만, 일정 부분은 **규칙 기반 검사**나 **자동화된 척도**로도 측정 가능합니다. 예를 들어 **DISTO**라는 연구에서는 참신한 **학습된 평가지표(learned metric)**를 개발하여, 질문-정답-지문 맥락을 모두 고려해 주어진 오답이 좋은 오답인지 점수화하였고, 그 점수가 인간 평가자의 오답 품질 평가와 높은 상관을 보였다고 합니다. 이는 **신경망 분류기**가 좋은 오답과 나쁜 오답을 구분하도록 훈련된 것으로, 향후 LLM 생성 오답의 품질 자동평가에 활용될 수 있습니다. 종합하면, **오답 품질**은 (a) **사람 평가**: (루브릭에 따른 점검이나 빈도 분석), (b) **LLM 평가**: (프롬프트를 통한 품질 진단), (c) **모델 기반 점수화**: (DISTO 같은 사전 학습된 척도) 등의 방법으로 구현할 수 있습니다.

- **정답 추측 용이성(Guessability) 평가:** 잘 만든 문항이라면 실제 지식을 갖지 않고 **찍기**로 정답을 맞힐 확률이 낮아야 합니다. 그러나 문항 설계상의 실수나 편향으로 인해 **정답이 드러나는 단서**가 존재하면, 학습 목표와 무관하게 정답을 맞히는 일이 생길 수 있습니다. 이를 **테스트 요령(test-wiseness)**에 의한 정답 맞힘이라고도 합니다. 정답 추측 용이성을 줄이기 위해서는 흔히 알려진 **문항 작성 오류**를 피해야 하는데, 평가에서는 이러한 오류의 존재 여부를 점검합니다. 대표적인 사례 및 측정 방법은 다음과 같습니다:

    - _보기 길이 편향:_ 정답 보기가 유독 길거나 자세한 경우, 학생들은 그 옵션이 정답일 가능성이 높다고 추측할 수 있습니다. 실제로 “정답이 가장 긴 보기인 경우”는 자동화된 품질 점검에서 주요 항목 중 하나로 포함되며, 검출 시 **감점 요소**로 처리됩니다. 평가 시 각 보기의 길이를 비교해 **정답이 다른 오답보다 훨씬 길거나 짧은지**를 검사하여 편향 여부를 확인합니다.

    - _언어 단서 및 논리 단서:_ 문제 지문이나 보기에 “항상, 절대, 전부” 같은 **절대 표현**이 정답이나 오답에만 들어가는 경우, 또는 문법적으로 질문문과 보기가 맞지 않아 **어색한 보기는 오답**으로 지레 짐작하게 되는 경우 등이 있습니다. 이러한 **논리적/문법적 단서**들은 규칙 기반으로 비교적 쉽게 검출 가능하므로, 정규식이나 알고리즘을 통해 “정답만 주어가 단수/복수 일치”와 같은 패턴을 찾아낼 수 있습니다. 예를 들어 사람이나 LLM에게 “각 보기가 질문의 어구와 문법적으로 일치하는지”를 체크하도록 하거나, **Item Writing Flaw** 체크리스트를 활용해 _논리적 단서_, _문법적 단서_ 존재 여부를 평가할 수 있습니다.

    - _기타 Test-wiseness 요소:_ “보기들 간 합치됨(convergence)”: 여러 보기가 서로 일부 겹치는 정보를 가지고 한 보기만 독립적인 경우, 독립적인 보기가 정답일 확률이 높다는 전략 등이 있습니다. 또 “둘 이상 정답처럼 보이는 보기”를 넣고 “모두 정답은 없음” 같은 **복합형**도 피해야 할 유형입니다. 이러한 잘 알려진 *문항 결함(flaws)*들은 **사전 정의된 규칙**으로 점검할 수 있습니다. 실제 LLM 생성 문항을 대량으로 평가한 사례를 보면, _Longest option correct_ (가장 긴 보기가 정답), _All of the above_ (보기에 “모두 해당” 포함 여부), _Gratuitous information_ (불필요한 정보 포함 여부) 등 여러 항목별로 룰베이스 검사를 수행하여 통과/실패 여부를 기록하고, 한 문항당 몇 개의 결함이 있는지 측정하기도 했습니다. GPT 기반으로 이러한 결함 검사를 한 연구에서는 GPT-4가 생성한 문항이 GPT-3.5보다 평균 Item Writing Flaw 개수가 적어 품질이 높다는 분석도 있었습니다. 정답 추측 용이성 평가는 이러한 다양한 패턴 검사 결과를 종합해, **해당 문항이 단서를 얼마나 제공하는지**를 0~100% 또는 5점 척도로 환산하는 식으로 정량화할 수 있습니다. 요약하면, **정답률 예측 가능성**(학습 내용이 아니라 문제형식으로 정답을 맞힐 가능성)은 _문항 작성 결함 체크리스트_를 통해 자동 측정하거나, LLM/사람에게 문항을 주고 “별도의 배경지식 없이 정답을 유추할 수 있나?”를 묻는 방식으로 평가합니다. 실제 한 의료 교육 연구에서도 약 2,400개의 MCQ를 분석하며 이런 **test-wiseness** 결함이 125건(전체의 약 22%) 발견되었다고 보고하여, 교육 현장의 문항들에서도 무시못할 비중으로 정답 단서 문제가 존재함을 지적하고 있습니다.

- **개념 포괄성(Concept Coverage) 평가:** 하나의 퀴즈(혹은 문항 세트)가 특정 단원이나 학습 주제의 **핵심 개념들을 얼마나 빠짐없이 다루는지**를 측정합니다. 예를 들어 2차 방정식 단원의 복습 퀴즈라면, 근의 공식을 묻는 문제, 판별식을 묻는 문제, 실제 해를 구하는 문제 등 여러 핵심 하위주제를 모두 포함하는지를 본다는 의미입니다. 이를 평가하려면 우선 각 문항이 어떤 **지식 요소(Knowledge Component)**나 **개념 태그**에 해당하는지 분류해야 합니다. 실제 교육 데이터셋(예: EdNet 등)에서는 각 문제에 해당 문제의 개념 태그(KC)가 부착되어 있고, 이를 통해 학생 모델을 훈련시키곤 합니다. 마찬가지로 LLM 생성 문제에도 해당 개념 태그를 달 수 있다면, **퀴즈의 개념 커버리지**를 “태그의 다양성” 혹은 “목표로 한 태그들의 포함 여부”로 정량화할 수 있습니다. 방법으로는:

    - _전문가 매핑:_ 교육과정 문서나 교수자 의도에 따라 이번 퀴즈에서 다뤄야 할 개념 리스트를 마련하고, 각 문항을 그 리스트와 대조하여 **포함/누락된 개념 수**를 셉니다. 예를 들어 목표 개념 5개 중 4개를 다뤘다면 80% 포괄성으로 평가할 수 있습니다.

    - _LLM 기반 분류:_ LLM에게 문항을 주고 “이 문항이 다루는 수학 개념은 무엇인가?”를 물어 태그를 추출하게 한 뒤, 사전에 기대한 개념들과 비교할 수 있습니다. 또는 “다음 퀴즈가 목표로하는 개념들을 모두 포함하고 있는가?”를 직접 물어볼 수도 있습니다.

    - _지식망 활용:_ 교육 지식망(knowledge graph)이 있다면, 각 문제를 지식망 위의 특정 노드(개념)에 매핑하고, 목표 영역의 모든 노드를 커버했는지 계산하는 방식도 생각할 수 있습니다.

    **퀴즈 체인** 형태의 문항 분해의 경우, 개별 문항들이 **부분 개념**을 나눠 맡게 됩니다. 이때는 체인을 이룬 모든 문항을 합쳤을 때 원래 문제의 해결에 필요한 개념이 완전히 커버되는지를 보는 것이 중요합니다. 각 단계 문항이 원문제의 특정 하위스킬을 담당하고 있으므로, 체인의 **포괄성**은 곧 **해결 절차상의 완전성**과 연결됩니다. 예를 들어 원문제가 “연립방정식 풀기”였고 체인이 이를 각각 “식1 정리”, “식2 정리”, “연립 해 구하기”로 나누었다면, 이 세 문항이 합쳐져 **연립방정식 풀이 전체 개념**을 포괄한다고 볼 수 있습니다. 만약 어떤 단계가 빠져 있다면 포괄성 측면에서 체인이 불완전한 것입니다. 요약하면, 개념 포괄성은 **설정한 학습 목표의 모든 요소를 퀴즈가 다루고 있는가**로 정의되며, 이를 위해 **개념 태깅과 비교분석** 기법을 활용합니다.

- **학습 목표 정렬(Alignment with Learning Objectives) 평가:** 개념 포괄성과 비슷하지만, 좀 더 상위 차원에서 **교육 목표나 성취기준과 문제 내용이 부합하는지**를 점검하는 지표입니다. 이는 **내용 타당도(content validity)** 평가와도 관련이 있습니다. 예를 들어 교육 과정 성취기준이 “학생이 일차함수의 기울기를 해석할 수 있다”라면, 그 목표 정렬도는 출제된 문제가 정말로 _기울기 해석_ 능력을 요구하는지를 보는 것입니다. **정렬**이 잘못되면 문제가 가르치지 않은 다른 개념을 묻거나, 중요하지 않은 부분을 다루거나, 혹은 지나치게 복잡하여 원래 목표를 평가하지 못하게 됩니다. 정량적 평가는 주로 **전문가 판단**에 의존하지만, 부분적으로는 **문서 분석**과 **자동 분류**로 지원될 수 있습니다. 방법은:

    - _학습 목표-문항 매핑표:_ 각 문항을 해당 단원의 목표나 성취기준 번호에 연결시키고, 올바른 연결인지 사람이 확인합니다. 여러 전문가가 **일치 여부를 점수**(예: 0=불일치, 1=일치, 0.5=부분일치)로 평가하여 평균을 내거나 Kappa로 합의도를 볼 수 있습니다.

    - _자연어 처리 활용:_ 목표 진술과 문제 텍스트를 임베딩하여 **유사도**를 계산하는 방법도 제안됩니다. 높은 유사도라면 문제에서 목표 키워드나 개념이 잘 나타난 것으로 간주하고, 너무 낮으면 정렬이 낮다고 추정할 수 있습니다.

    - _교육과정 체크리스트:_ **커리큘럼 매트릭스** 상에, 문제의 커버 영역이 엉뚱한 곳을 가리키지 않는지 검증합니다. 의료 교육 분야 연구에서는 학교의 MCQ들을 분석하여, 다수의 문항이 해당 커리큘럼 목표와 불일치하거나 부적절한 난이도의 내용을 다루는 등 **정렬 문제**가 있음을 밝히기도 했습니다. 2,400문항 중 약 23%에 해당하는 561개 문항에서 _교육과정 목표와의 부정합_이 발견되었고, 이를 **Test-wiseness** 결함, **Irrelevant difficulty**(의도와 무관한 어려움), 기타로 분류하여 문제를 진단하였습니다. 이러한 결과는 자동 생성 문항에서도 목표 정렬 검증이 필요함을 시사합니다.

    LLM이 생성한 문항의 경우, 프롬프트에 명시된 학습 목표를 제대로 반영하고 있는지 LLM 스스로 평가하게 하는 방법도 가능합니다. 예를 들어 “이 문제는 제시된 학습 목표 X를 평가하기에 적합한가?”를 GPT-4에게 물어보면, 근거와 함께 판단을 줄 수 있습니다. 다만 최종적으로는 사람 검토를 거치는 것이 안전합니다. 정량화는 목표와의 부합 정도를 0~1 사이로 점수화하거나, **일치/부분일치/불일치** 문항 비율을 산출하는 형태로 이루어집니다.

- **문항 명확성(Clarity) 평가:** 명확성은 문항의 **언어적 품질**과 관련됩니다. LLM이 생성한 문제의 경우 종종 문장 어투가 부자연스럽거나, 지시가 애매할 수 있으므로 이를 검토해야 합니다. **사람 평가**의 경우, 다수의 평가자에게 각 문항의 명확성을 5점 Likert 척도로 평가하도록 하여 평균을 내는 방법이 있습니다. 또는 **이해도 검사**로서, 학생이나 LLM에게 문제를 읽히고 “무엇을 묻는 문제인지 설명하게” 하여 올바르게 이해했는지 확인하는 간접 평가도 가능합니다. 예컨대 GPT-4에게 문항을 주고 “이 문제를 풀기 위해 무엇을 해야 하느냐?”를 물어서 엉뚱한 답을 내놓는다면, 문제 진술이 모호할 수 있습니다. 자동화 측면에서는 **자연어 규칙 검사**를 통해 명확성 저하 요소를 찾을 수 있습니다. 긴 문장이나 중첩된 절이 많은 문항, 이중부정 표현, 기호 남용 등을 점검해 점수를 매기는 식입니다. 실제 LLM 생성 MCQ 평가에서 **문항의 모호성(ambiguity)** 여부를 인턴 평가 기준에 포함시키기도 했습니다. 일반적으로 **명확성 지표**는 “명확함”으로 평가되면 1, 문제가 다소 헷갈리면 0으로 하여 전체 문항 중 명확한 문항 비율(%)로 나타내거나, 모호성 빈도를 문항당 평균으로 산출합니다. LLM을 활용하면 이러한 언어적 결함(예: 참조 불명확, 문법 오류)을 잡아내도록 지시하여 **자동 교정 점수**를 얻을 수도 있습니다.

- **정확성(Solution Correctness) 확인:** 이 부분은 사실 **평가 지표**라기보다는 전제 조건에 가깝지만, LLM 생성 문제의 경우 반드시 짚고 넘어가야 합니다. 문항 자체의 오류(예: 잘못된 지식 진술)나 **정답 키의 오류**는 교육적으로 치명적이므로, **100% 정확성**이 담보되어야 합니다. 이를 검증하는 방법으로, 1) **LLM 스스로 풀이**: 생성한 문제가 있다면 GPT-4 등에게 풀어보게 하여 답을 얻고, LLM이 제시한 정답과 원래 LLM이 낸 정답이 일치하는지 검사할 수 있습니다. 만약 일치하지 않으면 문제가 잘못 출제되었거나 모호할 확률이 높습니다. 2) **수학 솔버 활용**: Wolfram Alpha와 같은 수학 엔진에 문제를 넣어 결과를 비교하거나, 정답에 대해 심볼릭 검증을 하는 방법도 있습니다 (예: 정답이 어떤 방정식의 해라면 실제 대입 검증). 3) **전문가 검토**: 결국 가장 신뢰도 높은 방법은 수학 교사가 모든 문항과 정답을 직접 풀어 확인하는 것입니다. 자동화 도구는 보조 수단일 뿐, 완전한 보장을 주진 못합니다. 정량적으로는, 검증된 문항 비율, 발견된 오류 수 등의 형태로 기록할 수 있습니다 (예: 100문제 중 98문제가 정확 -> 98% 정확성).

요약하면, **지표별 평가**는 사람과 AI의 **이중 검사 체계**를 활용하는 것이 효과적입니다. 우선 LLM이나 규칙 기반 스크립트로 **자동 진단**을 실시해 난이도, 오답 품질, 문항 단서 여부 등을 1차 점검하고, 2차로 교육 전문가나 교사가 **루브릭에 따라 확인**하여 최종 품질을 담보하는 식입니다. 이러한 다면 평가를 통해 LLM 생성 퀴즈의 교육적 완성도를 높일 수 있습니다.

## 선택형 문제의 오답 난이도 및 유효성 평가

선택형 객관식 문항에서 **오답(distractor)**의 **난이도** 및 **유효성**은 별도의 심층 평가가 필요합니다. 여기서 _오답의 난이도_란, 그 오답이 얼마나 **학습자를 속이기 어려운지/쉬운지**를 의미하고, _유효성_이란 오답으로서 제 구실을 하는지 (즉, 실제 학습자를 효과적으로 속여 오답을 선택하게 만드는지) 여부를 뜻합니다.

오답 난이도를 판단하는 한 가지 방법은, **오답을 정답으로 가정했을 때 문제를 풀기 얼마나 어렵게 만들 수 있는가**를 생각해보는 것입니다. 예를 들어 어떤 오답이 실제로 정답처럼 매우 그럴듯하여, 그 오답을 선택한 학생들은 **자신이 정답을 골랐다고 착각**할 정도라면, 그 오답은 난이도가 높고 (쉽게 구별 못함) 유효한 distractor라고 볼 수 있습니다. 반면, 누가 보더라도 말이 안 되거나 문제와 관계없어 보이는 오답은 난이도가 낮은 오답(쉽게 걸러냄)이며 동시에 유효하지 않은 distractor라 할 수 있습니다.

**정량적 평가 기법:** 실제 응시자 데이터가 있다면, **문항 분석(item analysis)** 지표를 통해 오답의 기여도를 파악할 수 있습니다. 전통적으로 사용되는 값으로 **오답 효율(distractor efficiency)**이 있는데, 이는 각 오답이 일정 비율 이상 선택되었는지 여부로 계산합니다. 예를 들어 각 오답이 전체의 최소 5% 이상 선택되었다면 그 오답을 **기능적 distractor**로 보고, 그렇지 않고 5% 미만의 선택 빈도를 보이면 **비기능적 distractor**로 분류합니다. 문항 하나에 비기능적 오답이 2개 존재한다면, 4지선다의 경우 오답 효율 50% (절반만 기능)으로 평가할 수 있습니다. 이를 종합해 **Distractor 효율 지수**를 산출하면, 값이 낮을수록 (비기능 오답 多) 해당 문항의 오답 설계가 부실함을 나타냅니다. 더 나아가 오답 각각에 대해 **선택 확률**을 난이도와 연계해보면, 상위권 학생과 하위권 학생이 어떤 오답을 골랐는지도 분석할 수 있습니다. 이상적인 오답은 **하위권 학생들은 많이 선택하고 상위권 학생들은 거의 선택하지 않는** 형태를 띱니다. 이는 그 오답이 개념을 완전히 이해하지 못한 경우에만 주로 선택됨을 의미하며, **변별력** 확보에 기여합니다. 이러한 분석은 **문항변별도**와 연계되어 고전 검사이론(CTT)이나 문항반응이론(IRT)에서 다루는 영역입니다. 예컨대, 한 오답을 선택한 집단의 점수 평균이 정답을 맞힌 집단에 비해 유의미하게 낮다면, 해당 오답은 **학습자가 부족할 때 많이 선택하는 함정**으로 기능한 것이므로 좋은 distractor라 할 수 있습니다.

그러나 LLM이 문제를 생성하는 상황에서는 아직 실제 학생 데이터가 없을 수 있습니다. 이럴 때는 **모의 학습자 시뮬레이션**이나 **전문가 판단**으로 오답 난이도/유효성을 가늠합니다:

- _전문가 휴리스틱:_ 경험 많은 교사가 오답들을 보고 “이건 너무 쉬운 오답이네” 또는 “이 오답은 학생들이 헷갈릴 수 있겠다”라고 **직관적으로 평가**하는 방법입니다. 이를 조금 체계화하면, 전문가에게 각 오답별로 “매우 쉬운 오답(누구나 걸러냄)”부터 “매우 어려운 오답(정답처럼 속임)”까지 등급을 매기게 할 수 있습니다. 이렇게 얻은 등급을 수치화하여 평균 내거나 분포를 확인해, 문항별 distractor 난이도 프로파일을 생성합니다.

- _LLM 기반 시뮬레이션:_ LLM에게 **학생 역할을 수행**시켜보는 흥미로운 접근도 가능합니다. 예를 들어, “너는 개념 A를 착각하고 있는 학생이다”와 같은 프롬프트를 주어, 특정 오답을 유도하는 방향으로 LLM이 문제를 풀게 해볼 수 있습니다. 또는 “다음 중 정답이 무엇인지 확신이 없는 학생처럼 생각하고 선택지를 골라보라”는 식으로 프롬프트를 주어, LLM이 선택지를 평가하게 할 수도 있습니다. 여러 상태(잘못된 신념 or 부분적 지식)의 LLM을 병렬로 실행하여, 각 상태가 어떤 오답을 선택하는지 관찰하면 **오답 선택 분포**의 모의 데이터를 얻을 수 있습니다. 앞서 언급한 연구들에서는 GPT-3 등 QA 모델들을 다양한 버전(성능이 서로 다른 모델들)으로 준비하여 일종의 **가상 수험생 집단**으로 활용하기도 했습니다. 이를 통해 각 문항을 몇 개 모델이 맞히고 틀렸는지 집계하여 난이도를 예측하였듯이, 오답도 몇 개 모델이 선택했는지 볼 수 있습니다. 물론 이러한 LLM 기반 시뮬레이션이 실제 학생과 완전히 같다고 할 수는 없지만, **모델의 자체 평가 능력**과 **오답에 대한 언어적 plausibility 분석**을 활용하면 데이터 부재 상황에서 도움을 얻을 수 있습니다.

- _Distractor 전용 평가모델:_ 앞서 소개한 **DISTO 지표**처럼, 훈련된 모델이 오답의 품질을 점수로 내는 방식을 사용할 수도 있습니다. 이 경우 점수 자체가 오답의 매력도를 나타내므로, 일정 임계값 이상이면 **유효한 오답**, 이하면 **부실한 오답**으로 간주하는 식입니다.

정리하면, **오답의 난이도**는 "학습자가 이 오답을 오답으로 간파하기 어려운 정도"로 이해할 수 있으며, **유효성**은 "그 오답이 실제로 오답으로 기능하며 학습자의 이해 부족을 드러내게 하는지"로 요약됩니다. 두 가지 모두 **너무 낮으면** 문항 질이 떨어지므로, 위의 다양한 방법으로 이를 평가 및 향상시켜야 합니다.

## 문제 분해형 퀴즈(Quiz Chain)의 교육적 효과 평가

**퀴즈 체인**이란 복잡한 한 문제를 단계별 여러 소문항으로 나누어 출제하는 형식을 말합니다. 예를 들어 증명 문제라면 (1) 보조정리 확인, (2) 주요 단계 계산, (3) 최종 결론 도출처럼 세분화할 수 있습니다. 이러한 문제 분해형 접근의 **교육적 효과**는 이론적으로는 _학습 부담을 줄이고_ _사고 과정을 안내하여_ 이해를 돕는 장점이 있다고 여겨집니다. 이를 **정량화**한 사례나 방법론을 살펴보면 다음과 같습니다.

- **학습 성취도 향상 측정:** 가장 직접적인 방법은 **학습 전후 성과 비교**입니다. 문제를 분해하여 푼 그룹과 한꺼번에 원문제를 푼 그룹을 비교하는 실험을 통해, 사전-사후 테스트 점수 향상도나 후속 문제 해결 능력 차이를 분석합니다. 만약 퀴즈 체인이 효과적이라면, **분해형 퀴즈를 경험한 학습자**들이 이후 유사 문제를 더 잘 풀거나 개념 이해도 검사에서 높은 점수를 받을 것으로 기대됩니다. 실제로 많은 연구들이 **발견적 학습이나 스캐폴딩(scaffolding)**이 학습에 도움을 준다고 보고하고 있습니다. 예를 들어, 한 메타분석에서는 STEM 교육 맥락에서 문제 기반 학습에 대한 컴퓨터 스캐폴딩 제공이 **중간 정도의 긍정 효과**를 보였다고 합니다. 이는 여러 맥락에서 분해형/단계형 지도가 학업 성취에 일정 효과가 있다는 증거입니다.

- **즉각적 피드백 및 오류 교정:** 퀴즈 체인은 한 문제를 풀면서 중간중간 자신의 이해를 확인할 기회를 제공합니다. 이를 정량화하는 방법으로, **체인 내에서 발견된 오류율**이나 **재시도 횟수** 등을 들 수 있습니다. 예를 들어 원문제를 바로 풀 때는 어디서 실수했는지 알기 어려웠지만, 체인 문제를 통해 (1단계는 정답, 2단계 오답 발생) 같은 데이터가 있다면, 특정 단계에서 많이 틀리는 비율을 산출하여 **개념 장애물**을 파악할 수 있습니다. 교육 연구에서는 이러한 **중간 단계에서의 실패를 학습 기회로 활용**하는 것을 _productive failure_라고 하여, 일부러 학생들이 초기 단계에서 시행착오를 겪게 한 뒤 피드백을 주면 이후 학습 효과가 커진다는 결과도 있습니다. 구체적인 평가지표로는 각 단계별 **정답률 추이**(앞 단계 대비 뒷 단계 향상 여부), **힌트 요청 횟수**, **정답 공개 요청** 등의 로그 데이터를 비교합니다. 실제 ASSISTments 플랫폼에서 수행된 한 연구에서는, 힌트를 자유롭게 보는 그룹과 답이 틀리면 강제로 세부 단계(스캐폴드)를 밟게 한 그룹을 비교했는데, **성인 학습자**의 경우 강제 스캐폴드 그룹이 오히려 힌트 그룹보다 **정답을 포기하고 답을 알려달라는 비율**이 높았다는 결과도 있었습니다. 이는 성인 학습자에게는 세분화된 문제가 오히려 귀찮음을 유발할 수 있음을 시사합니다. 따라서 **학습자 특성에 따른 효용**도 평가해야 하는데, 이를 위해서는 실험군별로 로그 데이터(포기율, 완료율 등)나 사후 설문(만족도, 인지부하 등)을 수집하여 분석합니다.

- **전이 효과(transfer) 검사:** 분해형 퀴즈로 배운 지식이 **새로운 문제**에 적용되는지를 보는 것도 효과 평가에 중요합니다. 이를 위해 퀴즈 체인을 통해 특정 문제를 학습한 후, 관련 개념의 **변형 문제** 또는 **상황이 다른 문제**를 제시하여 얼마나 성공적으로 풀어내는지 측정합니다. 높은 전이 성과는 해당 퀴즈 체인이 **학습자의 개념 구성을 도왔다**는 증거입니다. 예를 들어, 분해형 퀴즈를 경험한 학생들이 동일 개념의 새로운 응용문제에서 정답률이 통제집단보다 유의미하게 높다면, 정량적으로 효과를 검증한 셈입니다. Sinha 등 연구에서도 사전 지식 없이 문제풀이 -> 이후 교수 단계에서, 중간에 명시적 스캐폴딩으로 틀려보는 경험을 한 그룹이 나중에 **근본 개념에 대한 이해와 새로운 문제 추론에서 더 나은 성과**를 냈다는 보고가 있습니다 (포스트테스트 점수 및 추론 질 향상).

- **학습 곡선 분석:** 여러 단계에 걸친 퀴즈는 하나의 문제를 풀기 위해 필요한 **하위 기능(skill)**들의 학습을 순차적으로 일으킵니다. 이때 얻을 수 있는 데이터로 각 학생의 단계별 소요 시간, 오답 횟수 등이 있습니다. 이를 **러닝 커브** 형태로 그려보면, 체인 진행 동안 **실수 감소 추세**나 **속도 향상**이 관찰될 수 있습니다. 예를 들어 1단계에서 평균 2회의 시도가 필요했던 것이 마지막 단계에서는 1회 만에 정답을 맞히는 식이라면, 학생들이 문제를 분해해 가며 숙달되어감을 나타냅니다. 이런 추세를 정량화하여, 체인이 없었을 경우와 비교하면 그 차이를 파악할 수 있습니다. (다만 한 문제 안에서의 학습효과를 보는 것이므로, 이는 미시적인 분석에 해당합니다.)

- **학습자 경험 및 인지 부하**: 교육적 효과는 성취도 뿐 아니라 **학습 경험의 질**도 포함합니다. 분해형 퀴즈가 학습자에게 **자신감 고취** 또는 **인지과부하 감소**에 기여했는지를 설문지나 경험샘플로 평가할 수 있습니다. 이를 수량화하기 위해 Likert 척도의 응답을 평균내거나 요인별 점수를 비교합니다. 예를 들어 "문제가 단계별로 나뉘어 있어 어려운 문제도 풀 수 있을 것 같았다(자기효능감)"에 대한 동의 정도를 실험/통제 집단간 비교하는 것입니다. 이러한 정성 요소까지 합쳐서 분해형 퀴즈의 **총체적 교육 효과**를 판단합니다.

요약하면, **문제 분해형 퀴즈의 효과**는 _학습 성과 데이터_(정답률, 전이점수 등)와 _행동 로그_(힌트/포기율, 시도 횟수 등), _학습자 피드백_을 아울러 분석함으로써 정량적으로 평가될 수 있습니다. 실제 많은 튜터링 시스템 연구가 이러한 A/B 테스트와 데이터 분석을 통해 스캐폴딩의 유용성을 탐색하고 있으며, 일반적으로는 **적절한 스캐폴딩은 학습에 긍정적**인 것으로 보고되지만, 과도한 세분화는 경우에 따라 **학습자 참여 저하**를 가져올 수도 있으므로, 그 효과를 면밀히 모니터링해야 함을 알 수 있습니다.

## 학습자 데이터 부재 시 모의 평가 (Simulated Learner Evaluation)

LLM이 새로 문제를 만든 상황에서는, 당장 **실제 학생들의 풀이 데이터**를 얻기 어려운 경우가 많습니다. 이럴 때 활용할 수 있는 방법이 **모의 학습자 시뮬레이션**을 통한 간접 평가입니다. 앞서 일부 언급한 내용을 확장하여, LLM이나 다른 AI를 **가상의 학생**으로 삼아 퀴즈를 풀어보게 함으로써 얻을 수 있는 정보들은 다음과 같습니다.

- **정답률 예측:** 여러 난이도 또는 서로 다른 설정의 모델에게 문제를 풀게 하고 정오답 결과를 수집하면, 인간 학습자의 정답률을 예측하는 지표로 사용할 수 있습니다. 이는 Uto 등의 연구에서 보인 방법으로, 소형 QA 모델부터 GPT-4 같은 대형모델까지 **여러 등급의 모델 군**을 테스트에 참여시켜, 각 문항을 맞힌 모델 비율을 계산하는 식입니다. 예를 들어 10개의 가상 학습자(모델) 중 3개만 정답을 맞혔다면 해당 문항의 예측 정답률을 30%로 보는 식입니다. 이후 인간 데이터가 확보되면 이 예측을 **보정(calibration)**하거나 비교 분석하여 신뢰성을 검증합니다.

- **오답 선택 경향:** 시뮬레이션을 하면 각 모델이 어떤 오답을 골랐는지도 알 수 있습니다. 만약 대다수 모델이 특정 오답에 몰린다면, 그 오답에는 문제의 함정과 관련된 뭔가가 있다고 추정할 수 있습니다. 반대로 인간이 절대 선택하지 않을 것 같은 오답을 모델들도 무시한다면, 해당 오답은 개선 필요가 있겠습니다. LLM은 아무 배경지식도 없거나 잘못된 지식을 가진 것처럼 프롬프트를 주어 특정 오답을 선택하도록 **컨트롤**할 수도 있기 때문에, 이를 통해 오답별 _가상 오답률_ 분포를 얻어 앞서 말한 distractor 유효성 평가에 활용합니다.

- **풀이 과정 분석:** LLM을 단순히 정답만 고르게 하는 대신, **풀이 과정을 출력**하게 할 수도 있습니다. 예를 들어 "학생" 역할을 하는 GPT-4에게 문제를 풀라고 지시하면, step-by-step으로 추론 경로를 보여줍니다. 이때 **어디서 잘못된 가정이나 실수를 하는지**를 관찰하면, 실제 학생들이 빠질 법한 함정과 상당 부분 유사한 패턴을 발견할 수 있습니다. 예를 들어 GPT-4가 문제를 풀다 어떤 잘못된 개념을 적용해 오답을 도출했다면, 해당 문항이 의도한 _오개념 유발 포인트_가 제대로 작동한다고 볼 근거가 됩니다. 이러한 **LLM-as-student** 접근은 인간 데이터 전 무단계에서 문항을 개선하는 데도 도움됩니다. LLM의 풀이 로그를 많이 모으면, 마치 여러 학생들의 풀이를 본 것처럼 **오류 유형 분류**나 **추론 난이도** 등을 파악할 수 있습니다.

- **LLM을 이용한 자동 평정(Grader로 활용):** 이번 섹션의 맥락과 약간 다르지만, 모의 평가의 한 형태로 **LLM을 채점자/평가자**로 활용하는 방법도 있습니다. 즉, 학생 데이터가 없을 때 LLM에게 객관적인 평가자 시각에서 문항을 평가하게 하는 것입니다. 예를 들어 GPT-4에게 특정 문항에 대해 앞서 정의한 여러 지표(난이도, 품질 등)에 대해 5점 만점 평가와 코멘트를 달라고 요청할 수 있습니다. 실제 한 연구에서는 GPT-4 등 LLM이 사람 전문가 수준으로 문항의 **결함(Item Writing Flaw)**을 찾아내는지 실험하였는데, 문항 당 결함 발견 개수나 분류에서 일정 부분 사람 평가와 일치하는 경향을 보였습니다. 이러한 결과를 바탕으로, LLM의 평가를 완전히 신뢰할 수는 없지만 **1차 필터링**이나 **보조적 평가**로 활용 가능함을 시사합니다. 예를 들어 LLM이 “이 문항은 난이도 중간이고, 오답 하나가 부자연스럽습니다”라고 평가하면, 이후 사람이 검토할 때 초점을 그 부분에 맞춰 살펴볼 수 있습니다.

- **지식 추적 모델 활용:** EdNet이나 ASSISTments 같은 대규모 데이터로 훈련된 **Knowledge Tracing** 모델을 활용하는 방안도 있습니다. 이러한 모델은 **가상의 학생**을 시뮬레이션하여 새로운 문제에 대한 반응을 생성할 수 있습니다. 예를 들어 Bayesian Knowledge Tracing(BKT) 모델이 있다면, 특정 개념에 대한 숙련도를 0.6으로 설정한 가상의 학생이 본 문항을 맞힐 확률을 계산해줍니다. 혹은 RNN 기반의 Deep Knowledge Tracing 모델에 새 문항 벡터를 투입하여 예측 성능을 측정하는 방법도 가능할 것입니다. 이는 일종의 **모델 기반 예상 반응 생성**으로 볼 수 있으며, 난이도 예측과 유사하게 활용됩니다.

전체적으로, 실제 학생 데이터 없이도 **다양한 AI 에이전트나 모델을 동원**하여 문항에 대한 피드백을 얻는 것이 가능합니다. 다만, 이러한 간접 평가는 어디까지나 **참조 지표**로 활용해야 하며, 실제 학생들의 반응을 완벽히 대체하지 못한다는 한계를 인지해야 합니다. 최종적으로는 가능하면 소규모라도 파일럿 테스트를 통해 **현장 데이터**를 수집하고, 모의 평가 결과와 대조·보완하면서 LLM 생성 퀴즈의 품질을 개선하는 방향이 권장됩니다.

## 관련 시스템 및 연구 사례의 평가 방식

마지막으로, 실제 교육 현장이나 연구에서 **자동 생성 문제 평가** 또는 **학생 반응 데이터 활용**이 어떻게 이루어지고 있는지 몇 가지 사례를 들어보겠습니다:

- **Khan Academy**: Khan Academy는 대규모 사용자 기반을 통해 문제 난이도와 학습자 성취 데이터를 확보하고 있는 플랫폼입니다. Khan Academy에서는 문제를 풀었을 때 축적되는 방대한 로그 데이터로 각 문제의 **정답률**을 지속적으로 추적하고, 이를 기반으로 **난이도 레이블**을 조정하거나, 개인 맞춤 문제 추천에 반영합니다. 또한 각 문제에는 연관된 **지식 태그**(예: _Addition & subtraction_)가 매핑되어 있어서, 학생의 답변 이력을 통해 **마스터리 수준**을 판정하는 Knowledge Tracing을 수행합니다. 이를 통해 만약 LLM이 문제를 생성하여 추가한다면, 기존 시스템에서는 **유사한 태그**의 문제 정답률과 비교하여 난이도를 예상하거나, 초기 소수 학생들에게 시험 출제하여 결과를 보고 난이도를 보정하는 식의 평가를 할 수 있습니다. Khan Academy와 같은 시스템은 **A/B 테스트**를 자주 활용하는데, 예를 들어 새로 생성된 문제 세트를 한 그룹에게, 기존 문제 세트를 다른 그룹에게 제공하고, **학습 진도, 완료율, 재시도율** 등의 지표를 비교하여 새 문제의 효과를 검증할 수 있습니다. 또한 Khan Academy는 **문항에 대한 학생 피드백**도 받곤 하는데, 만약 다수 학생이 “문제가 이해되지 않는다” 등의 피드백을 남기면 명확성 측면에서 개선 신호로 삼습니다.

- **ASSISTments**: 미국에서 중등 수학 교육에 널리 쓰이는 ASSISTments 플랫폼은 교사들이 만든 문제와 다양한 **튜터링 기능(힌트, 단서 제공)**을 지원하며 연구에도 많이 이용됩니다. 이 플랫폼에서는 각 문제에 대한 **세밀한 로그**를 남겨, 학생이 틀릴 경우 어떤 **힌트**를 보았는지, **스캐폴딩 문제**(sub-problem)를 풀었는지 등을 기록합니다. 이를 활용한 연구들에서는, 예를 들어 **Scaffold vs. Hint** 조건으로 실험을 진행하여 두 그룹의 **학습 결과와 행동 패턴**을 비교하였습니다. Zhou 등의 연구에서는 scaffolding형 문제를 받은 그룹이 hint형보다 **정답을 바로 포기하고 답을 요청**하는 비율이 높았는데, 이 데이터는 로그에서 _“bottom-out hint”_ 요청 횟수로부터 산출되었습니다. 또한 ASSISTments에서는 **Mastery Speed**라는 지표를 쓰는데, 이는 한 스킬을 완전히 익히는 데 학생이 푼 문제 수를 의미합니다. LLM이 생성한 유사문제를 **스킬 빌더 세트**로 제공할 경우, **Mastery Speed 단축 여부**를 통해 그 문제 세트의 학습 효율을 측정할 수 있습니다 (예: 기존엔 평균 7문제 걸리던 것을 5문제로 줄였다면 효과적). 더불어, ASSISTments 데이터는 연구자들이 **문항 난이도 모델링**을 할 때도 쓰입니다. 과거 연구에서 이 플랫폼 데이터를 기반으로 한 **클래식 테스트 이론** 분석에서는, 좋은 문항이란 _0.3~0.9 사이의 적절한 난이도_, _0.2 이상 높은 변별도_를 갖고, _오답 효율 높음_ 등의 특징이 있음을 밝히기도 했습니다. 이러한 기준은 LLM 생성 문항을 걸러내는 평가 기준으로 적용 가능합니다.

- **EdNet (산타)**: EdNet은 한국의 에듀테크 기업에서 공개한 대규모 학습자 상호작용 데이터셋으로, 약 780k명의 학생이 2년간 남긴 **1억 건 이상의 문제 풀이 로그**를 담고 있습니다. 이 데이터셋에는 각 문제에 대한 **지문, 보기, 정답, 해설** 뿐 아니라, 학생별 **정오답, 소요시간, 클릭스트림** 등이 포함되어 있어, 문항 품질을 다각도로 분석할 수 있습니다. 연구자들은 EdNet을 활용해 **딥러닝 기반 지식 추적**을 실험하거나, **문항 추천 알고리즘**을 개발하고 있습니다. EdNet 내에 포함된 문제들은 모두 **태그된 교육과정 요소**와 연결되어 있어, 특정 태그의 문항들이 어떻게 학생에게 받아들여지는지를 볼 수 있습니다. 예를 들어 태그 X의 문항들이 전반적으로 정답률이 낮다면 해당 개념의 교수법이나 문제 표현에 어려움이 있음을 시사할 수 있습니다. LLM이 생성한 문제라도 동일한 태그를 달아 EdNet의 과거 유사 문항과 비교하면, **예상 난이도**나 **오답 패턴**을 미리 유추해볼 수 있을 것입니다. 또한 EdNet처럼 리치한 데이터가 없더라도, 소규모 학교 단위로 **문항반응이론(IRT)**을 적용해 각 문항의 **난이도 파라미터(b)**와 **변별도 파라미터(a)**를 추정하고 관리하는 시스템도 있습니다. 이러한 시스템에서는 정기적으로 시험 결과를 분석해 문항 특성을 업데이트하고, 새로 출제된 문항도 시험에 포함하여 그 **모수**를 추정함으로써 시험지의 품질을 관리합니다. 만약 LLM 생성 문항을 실제 시험에 활용한다면, **pilot 테스트**를 통해 IRT 모수를 구하고 기존 문항들과 비교하여 성능이 떨어지는 (예: 변별도가 낮거나 추측도(c)가 높은) 문항은 퇴출시키는 방식으로 품질을 유지할 수 있을 것입니다.

- **기타 연구 동향**: 최근에는 LLM의 도움으로 **문항 생성뿐 아니라 평가까지 자동화**하려는 시도가 늘고 있습니다. 예를 들어, OpenAI의 GPT-4를 활용해 생성한 시험 문항에 대해 다시 GPT-4로 **해설과 채점 루브릭**을 작성하게 하거나, **학생 풀이 단답형 채점을 LLM으로** 처리하는 연구들도 진행되고 있습니다. 이런 흐름에서, 앞서 말한 LLM-as-a-Grader 기법이 더 발전하면 LLM이 생성 -> LLM이 평가 -> 사람 검증의 **반자동 루프**로 교육 콘텐츠 개발 주기를 크게 단축할 가능성도 있습니다. 다만, 완벽한 자동 평가는 아직 신뢰성 문제로 제한적 활용에 그치고 있습니다. 그래서 인간 전문가 평가(Human-in-the-loop)를 완전히 배제하기보다는, **전처리 단계에서 LLM 평가로 걸러내고, 최종 확인은 사람이 하는** 식으로 혼합 운영하는 사례가 많습니다.

## 결론

요약하면, **LLM이 생성한 K-12 수학 퀴즈의 품질**을 정량적으로 평가하기 위해서는 다각도의 지표와 방법을 적용해야 합니다. 난이도, 오답 품질, 정답 추측 용이성, 개념 포괄성, 목표 정렬 등의 지표를 통해 문제의 교육적 유용성을 측정하고, 이를 위해 LLM 기반 평가, 인간 전문가 루브릭, 그리고 실제/모의 학습자 데이터 분석을 활용할 수 있습니다. Distractor의 경우 특히 **매력적인 오답**의 존재가 문항의 변별력을 좌우하므로, 인간과 AI의 관점에서 모두 면밀히 살펴봐야 합니다. 또한 복잡한 문제를 퀴즈 체인으로 분해하는 접근은 학습 효과 면에서 장단점이 있으며, 이를 검증하려면 **학습자 성취도 향상, 오류 수정 빈도, 전이능력 향상** 등을 지표로 삼아 실험적으로 평가해야 합니다. 실제 데이터가 없을 때는 LLM을 가상의 학생이나 평가자로 활용하여 간접 지표를 얻고, 추후 현실 세계의 학생 데이터와 교차 검증함으로써 평가의 정확도를 높일 수 있습니다.

마지막으로, Khan Academy, ASSISTments, EdNet 등의 실제 시스템에서 사용되는 난이도 추적, 지식 태그, 로그 분석, A/B 테스트 등의 기법은 LLM 생성 퀴즈의 품질 평가에도 그대로 응용 가능합니다. 관련 연구들은 **AI가 만든 문항도 사람에 뒤지지 않는 품질**을 가질 수 있음을 시사하면서도, 여전히 **자동 평가의 한계와 인간 검증의 중요성**을 강조합니다. 그러므로 향후에는 LLM의 **문항 생성-평가-개선** 사이클을 효과적으로 설계하여, 교사들의 부담을 덜면서도 신뢰할 수 있는 학습 콘텐츠를 제공하는 것이 목표가 될 것입니다.
