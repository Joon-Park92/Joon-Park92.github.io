---
publish: true
uuid: a76b9851-0a4b-498e-b949-012360173f1d
---

## 1. TL;DR

- 결정 이론은 불확실한 상황에서 최적의 결정을 내리는 방법을 제공함.
- 오분류율 최소화, 기대 손실 최소화, 거부 옵션 등 다양한 접근법이 있음.
- 추론과 결정을 분리하는 방식과 통합하는 방식 모두 장단점이 있음.

## 2. 결정 이론의 기본 개념

- 결정 이론은 확률론과 결합하여 불확실한 상황에서 최적의 결정을 내리는 방법을 제공함.
- 입력 벡터 x와 예측 변수 벡터 t의 결합확률분포 p(x,t)가 주어졌을 때, 이를 바탕으로 결정을 내림.
- 결정 영역과 결정 경계 개념을 도입하여 입력 공간을 분할함.

<details markdown="1">
<summary>결합확률분포와 베이즈 정리</summary>

결합확률분포 p(x,t)는 다음과 같이 베이즈 정리를 통해 표현할 수 있음:

$$
p(t|x) = \frac{p(t) \cdot p(x|t)}{p(x)} = \frac{p(x,t)}{p(x)}
$$

여기서 p(t|x)는 사후확률, p(t)는 사전확률, p(x|t)는 우도, p(x)는 증거임.
</details>

## 3. 오분류율 최소화

- 가장 단순한 접근법으로, 오분류 확률을 최소화하는 것이 목표임.
- 사후 확률이 가장 높은 클래스로 분류하는 것이 최적의 결정임.
- 이진 분류의 경우, 결정 경계는 두 클래스의 사후 확률이 같아지는 지점임.

<details markdown="1">
<summary>오분류 확률 수식</summary>

오분류 확률은 다음과 같이 표현할 수 있음:

$$
p(\text{mistake}) = P(x \in \mathcal{R_1}, \mathcal{C_2}) + P(x \in \mathcal{R_2}, \mathcal{C_1})
$$

$$
= \int_{\mathcal{R_1}} p(x, \mathcal{C_2})dx + \int_{\mathcal{R_2}} p(x, \mathcal{C_1})dx
$$

여기서 $\mathcal{R_1}$과 $\mathcal{R_2}$는 각각 클래스 1과 2에 대한 결정 영역임.
</details>

<details markdown="1">
<summary>사후 확률이 가장 높은 클래스로 분류하는 것이 최적의 결정인 이유</summary>

- 오분류 확률 정의 - 오분류 확률은 잘못 분류된 데이터의 확률
- 오분류 확률은 다음과 같이 표현됨
    - $p(\text{mistake}) = \sum_{k} \int_{R_k} \sum_{j \neq k} p(C_j, x) dx$
- 최적화 목표: 오분류 확률을 최소화하는 결정 규칙을 찾는 것이 목표.
- 적분 내부 최소화:
    - 각 x에 대해 적분 내부를 최소화하면 전체 적분도 최소화된다.
    - 즉, 각 x에 대해 다음을 최소화해야 함
        - $\sum_{j \neq k} p(C_j, x)$
- 전체 확률의 법칙 적용
    - $\sum_{j} p(C_j, x) = p(x)$
    - 따라서, $\sum_{j \neq k} p(C_j, x) = p(x) - p(C_k, x)$
- 최소화 조건:
    - $p(x) - p(C_k, x)$를 최소화하는 것은 $p(C_k, x)$를 최대화하는 것과 동일
- 베이즈 정리 적용:
    - $p(C_k, x) = p(C_k|x)p(x)$
    - 여기서 $p(x)$는 모든 클래스에 대해 동일하므로, $p(C_k|x)$를 최대화하는 것이 곧 $p(C_k, x)$를 최대화하는 것과 같다.
- 결론:
    - 따라서, 각 x에 대해 사후 확률 $p(C_k|x)$가 가장 높은 클래스 k로 분류하는 것이 오분류 확률을 최소화하는 최적의 결정.

이 증명은 각 데이터 포인트에 대해 가장 높은 사후 확률을 가진 클래스를 선택함으로써 전체적인 오분류 확률을 최소화할 수 있음을 보여준다.
이는 직관적으로도 이해할 수 있는데, 각 결정에서 가장 가능성 있는 선택을 하면 전체적으로 실수를 최소화할 수 있기 때문
</details>

## 4. 기대 손실 최소화

- 오분류의 비용이 다를 수 있는 상황에 적용 가능함.
- 손실 함수(비용 함수)를 도입하여 각 결정에 따른 비용을 측정함.
- 기대 손실을 최소화하는 결정 규칙을 찾는 것이 목표임.

<details markdown="1">
<summary>기대 손실 수식</summary>

기대 손실은 다음과 같이 표현할 수 있음:

$$
\mathbb{E}[L] = \sum_k \sum_j \int_{\mathcal{R_j}} L_{kj} \cdot p(x, \mathcal{C_k})dx
$$

여기서 $L_{kj}$는 실제 클래스가 k일 때 j로 분류할 경우의 손실임.

최적의 결정 규칙은 다음 값을 최소화하는 j를 선택하는 것임:

$$
\sum_k L_{kj} p(\mathcal{C_k}|x)
$$
</details>

## 5. 거부 옵션

- 모든 클래스의 사후 확률이 낮은 경우, 결정을 유보하는 옵션을 제공함.
- 임계값을 설정하여 불확실한 경우 전문가의 판단을 요청할 수 있음.

## 6. 추론과 결정의 분리

- 분류 문제 (Classification) 를 추론 단계와 결정 단계로 나눌 수 있음.
- 세 가지 주요 접근 방법
    1. 생성 모델 (Generative model)
    2. 판별 모델 (Discriminative model)
    3. 판별 함수 (Discriminant function)
- 실제 응용에서는 문제의 특성, 데이터의 양과 차원, 계산 자원, 해석 가능성 요구 등을 고려하여 적절한 모델을 선택하거나 여러 모델을 앙상블하여 사용

### 6.1. 방법론의 특징

- 생성 모델
    - 생성 모델은 각 클래스의 데이터 생성 과정을 모델링.
    - p(x) 계산을 통해 이상치 탐지 등 추가적인 분석 가능.
    - 조건부 확률 밀도 p(x|C_k)와 사전 확률 p(C_k)를 추정, 베이즈 정리를 사용하여 사후 확률 계산, 입력과 출력의 결합 확률 분포를 모델링함.
    - 장점: 입력 공간의 구조에 대한 정보 제공, 이상치 탐지 가능
    - 단점: 높은 차원의 입력에 대해 많은 학습 데이터가 필요함.
- 판별 모델
    - 사후 확률 p(C_k|x)를 직접 추론
    - 결정 이론 적용 가능.
    - 장점: 분류에 필요한 정보만 모델링하여 효율적
    - 판별 모델은 클래스 간 결정 경계를 직접 학습.
- 판별 함수
    - 입력 x에서 직접 클래스 레이블을 예측하는 함수 f(x) 학습
    - 사후 확률 정보 없음
    - 장점: 가장 직접적이고 단순한 접근법
    - 단점: 확률 정보 제공 불가

### 6.2. Examples

<details markdown="1">
<summary>1. 생성 모델 (Generative model) 예시</summary>

- 나이브 베이즈 분류기
    - 스팸(S)과 정상(H) 이메일의 비율 계산: p(S), p(H)
    - 각 단어가 스팸과 정상 이메일에 나타날 확률 계산: p(word|S), p(word|H)
    - 새 이메일에 대해 p(S|email) vs p(H|email) 비교

     ```python
     def classify_email(email, word_probs, p_spam):
         log_prob_spam = log(p_spam)
         log_prob_ham = log(1 - p_spam)
         for word in email:
             if word in word_probs:
                 log_prob_spam += log(word_probs[word][0])
                 log_prob_ham += log(word_probs[word][1])
         return log_prob_spam > log_prob_ham
     ```

- 가우시안 판별 분석 (이미지 분류)
    - 각 클래스의 평균과 공분산 행렬 계산
    - 새 이미지에 대해 각 클래스의 확률 밀도 계산
    - 가장 높은 확률을 가진 클래스 선택

     ```python
     from scipy.stats import multivariate_normal

     def classify_image(image, means, covs, priors):
         probs = [multivariate_normal.pdf(image, mean=mu, cov=cov) * prior
                  for mu, cov, prior in zip(means, covs, priors)]
         return np.argmax(probs)
     ```

</details>

<details markdown="1">
<summary>2. 판별 모델 (Discriminative model) 예시</summary>

- 로지스틱 회귀
    - 각 클래스에 대한 로지스틱 함수 학습
    - 새 이미지에 대해 각 클래스의 확률 계산
    - 가장 높은 확률을 가진 클래스 선택

     ```python
     from sklearn.linear_model import LogisticRegression

     model = LogisticRegression()
     model.fit(X_train, y_train)
     probabilities = model.predict_proba(X_test)
     predictions = model.predict(X_test)
     ```

- 신경망
    - 다층 퍼셉트론 구조 정의
    - 손실 함수로 크로스 엔트로피 사용
    - 역전파를 통한 가중치 학습
    - 소프트맥스 함수로 클래스 확률 출력

    - 코드 스니펫 (PyTorch 사용):

     ```python
     import torch.nn as nn

     class DigitClassifier(nn.Module):
         def __init__(self):
             super().__init__()
             self.model = nn.Sequential(
                 nn.Linear(784, 128),
                 nn.ReLU(),
                 nn.Linear(128, 10)
             )

         def forward(self, x):
             return self.model(x)

     model = DigitClassifier()
     criterion = nn.CrossEntropyLoss()
     optimizer = torch.optim.Adam(model.parameters())
     ```

</details>

<details markdown="1">
<summary>3. 판별 함수 (Discriminant function) 예시</summary>

- 결정 트리
    - 특성을 기반으로 트리 구조 생성
    - 각 노드에서 가장 정보 이득이 높은 특성으로 분할
    - 리프 노드에 도달할 때까지 반복
    - 새 데이터의 클래스는 해당 리프 노드의 다수 클래스로 결정

     ```python
     from sklearn.tree import DecisionTreeClassifier

     model = DecisionTreeClassifier()
     model.fit(X_train, y_train)
     predictions = model.predict(X_test)
     ```

- K-최근접 이웃 (K-NN)
    - 새로운 데이터 포인트와 모든 훈련 데이터 간의 거리 계산
    - 가장 가까운 K개의 이웃 선택
    - 다수결로 클래스 결정

     ```python
     from sklearn.neighbors import KNeighborsClassifier

     model = KNeighborsClassifier(n_neighbors=3)
     model.fit(X_train, y_train)
     predictions = model.predict(X_test)
     ```

</details>

## 7. 사후 확률의 이점

1. 위험 최소화
    - 손실 행렬 변경 시 쉽게 새로운 결정 규칙 적용 가능

2. 거부 옵션
    - 불확실한 경우 결정을 유보할 수 있는 기준 제공

3. 클래스 사전 확률 보정
    - 학습 데이터와 실제 분포의 차이 보정 가능
    - 예: 희귀 질병 진단 시 인위적으로 균형 잡힌 데이터셋 사용 후 실제 확률로 보정

4. 모델 결합
    - 여러 독립적인 모델의 출력을 체계적으로 결합 가능
    - 예: X-ray와 혈액 검사 결과 결합

### 7.1. 모델 결합 예시: 조건부 독립 가정

- 복잡한 문제를 더 작은 하위 문제로 분할하여 해결 가능.
- 각 하위 문제의 결과를 확률적으로 결합.

<details markdown="1">
<summary>조건부 독립 가정을 사용한 모델 결합</summary>

X-ray 이미지(x_I)와 혈액 검사(x_B) 데이터가 주어졌을 때:

1. 조건부 독립 가정:
   $$p(x_I, x_B | C_k) = p(x_I | C_k)p(x_B | C_k)$$

2. 베이즈 정리 적용:
   $$p(C_k | x_I, x_B) \propto p(x_I, x_B | C_k)p(C_k)$$

3. 조건부 독립 가정 적용:
   $$p(C_k | x_I, x_B) \propto p(x_I | C_k)p(x_B | C_k)p(C_k)$$

4. 다시 베이즈 정리 사용:
   $$p(C_k | x_I, x_B) \propto \frac{p(C_k | x_I)p(C_k | x_B)}{p(C_k)}$$

이는 naive Bayes 모델의 한 예시임.
주의: 결합 주변 분포 p(x_I, x_B)는 여전히 인수분해되지 않음.
</details>