---
publish: true
uuid: 55f9b832-4723-4ac6-aba1-c5222f275773
---

# 다단계 추론과 LLM 파이프라인에서 오류 전파 완화 전략

대규모 언어 모델(LLM)을 여러 단계로 활용하는 **파이프라인**이나 **에이전트** 구조에서는, 초기에 발생한 작은 오류가 이후 단계에 누적되어 **결과를 크게 그르칠 위험**이 있습니다. 실제로 LLM은 토큰 단위로 순차 출력을 생성하므로 각 단계의 1% 미만 실수 확률도 200번째 토큰쯤엔 87% 오차 가능성으로 **기하급수적 증가**할 수 있다고 보고되었습니다 ([Compounding Error Effect in Large Language Models: A Growing Challenge - Wand AI](https://wand.ai/blog/compounding-error-effect-in-large-language-models-a-growing-challenge/#:~:text=To%20understand%20this%20better%2C%20consider,error%20by%20the%20200th%20token)). 따라서 복잡한 문제를 단계별로 해결할 때 **오류가 증폭**되지 않도록 설계하는 것이 매우 중요합니다. 아래에서는 **오류 전파를 줄이는 설계 패턴과 피드백 루프**, 그리고 **할루시네이션(hallucination)**이라 불리는 거짓 정보 생성을 완화하기 위한 빅테크 기업들의 접근법을 살펴봅니다.

## 다단계 LLM 파이프라인 설계와 오류 축적 방지

### 1. 문제 분해와 단계별 검증

복잡한 요청은 한 번에 끝까지 답변하기보다 **중간 단계로 분해 후 처리**하고 각 단계 출력을 검증하는 전략이 효과적입니다 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=To%20enhance%20output%20reliability%2C%20we,blog%20on%20prompt%20optimization%20techniques)). 예를 들어 질문을 하위 질문들로 나누고, 각 하위 답을 사실 확인한 뒤 종합하면 오류가 누적될 가능성이 낮아집니다. 필요하다면 **필요한 부분만 전문 도구에 위임**하기도 합니다. (예: 수식 계산은 계산기 API 사용, 사실 검색은 검색 엔진 사용 등) 이렇게 하면 LLM이 모든 것을 추론하며 발생할 수 있는 실수를 줄일 수 있습니다. 또한 **중간 결과에 대한 기대 조건**(예: “이 값은 음수일 수 없다” 등)을 명시하고, LLM이 이 조건을 만족하는지 검사하는 **밸리데이션 단계**를 넣어 잘못된 방향으로 진행되지 않도록 설계할 수 있습니다. 이러한 모듈화는 한 단계의 오류가 다음 단계에 전파되기 전에 **격리 및 수정**할 기회를 제공합니다.

### 2. 자기 검토(self-check)와 피드백 루프

LLM 에이전트 구조에서는 **자기 성찰**이나 **비평자 모듈**을 통해 출력 오류를 감지·수정하는 피드백 루프를 구현할 수 있습니다. 이는 인간 작문 과정의 교정처럼, 모델이 초기 응답을 생성한 뒤 스스로 **비판적으로 검토**하고 수정안을 내도록 유도하는 방법입니다 ([Reflection Agents](https://blog.langchain.dev/reflection-agents/#:~:text=Reflection%20is%20a%20prompting%20strategy,information%20such%20as%20tool%20observations)). 예를 들어 한 LLM이 답변 생성(`Generate`)을 하면, 별도의 **반성 단계(`Reflect`)**에서 같은 또는 다른 LLM이 그 답변의 결점이나 사실 오류를 지적하게 합니다. 그런 다음 오류가 지적된 부분을 반영하여 다시 답변을 개선(`Generate`)하는 식으로 **여러 차례 반복**해 최종 출력을 얻습니다 ([Reflection Agents](https://blog.langchain.dev/reflection-agents/#:~:text=Reflection%20is%20a%20prompting%20strategy,information%20such%20as%20tool%20observations)) ([Reflection Agents](https://blog.langchain.dev/reflection-agents/)). 이렇게 **생성과 반성의 루프**를 도입하면 한 번의 시도에서 생긴 오류를 그다음 단계에서 바로잡을 수 있어 출력 품질과 신뢰성이 향상됩니다. 실제 LangChain 에이전트 구현 등에서도 이러한 *“Reflect and Refine”* 패턴을 사용하여 LLM이 **스스로 피드백을 반영**하도록 함으로써 성공률을 높인 사례가 보고됩니다. OpenAI 또한 2024년 내부 연구에서 GPT-4로 구성된 *“CriticGPT”* 모델을 도입하였는데, 이는 ChatGPT의 응답을 검토하여 오류를 **지적하는 비평자 모델**로 활용되었습니다 ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=OpenAI%27s%20solution%20is%20to%20create,the%20initial%20prompt%20for%20context)). CriticGPT는 사람 검토자가 “완벽함”으로 평가한 답변에서도 버그를 추가로 잡아내는 등 성능을 보여, **사람+AI 결합 평가**가 단일 인간 평가보다 효과적임을 시사했습니다 ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=In%20putting%20CriticGPT%20through%20its,flawless)) ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=CriticGPT%20is%20not%20a%20replacement,responses%20may%20lower%20its%20reliability)). 다만 **AI 비평자 역시 완벽하지 않아** 자체적으로 환각할 위험이 있으므로(모델이 사실과 다른 비평을 하는 등) 최종적으로는 인간과 AI 평가를 **병행**하는 접근이 권장됩니다 ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=CriticGPT%20is%20not%20a%20replacement,responses%20may%20lower%20its%20reliability)).

### 3. 다중 시도 및 앙상블(Self-Consistency 등)

단일 경로로 한 번 추론하는 대신, **여러 경로로 반복 시도**한 뒤 가장 그럴듯한 답을 선택하면 오류 확률을 줄일 수 있습니다. 이를테면 **Self-Consistency** 기법은 동일 질문에 대해 LLM으로부터 **복수의 추론 경로**(Chain-of-Thought 결과)를 샘플링하고, 그 중 **가장 빈번하게 도출된 결론**을 최종답으로 선택합니다 ([Self-Consistency with Chain of Thought (CoT-SC) | by Johannes Koeppern | Medium](https://medium.com/@johannes.koeppern/self-consistency-with-chain-of-thought-cot-sc-2f7a1ea9f941#:~:text=Let%E2%80%99s%20talk%20about%20a%20prompting,the%20source%20proves%20with%20experiments)) ([Self-Consistency with Chain of Thought (CoT-SC) | by Johannes Koeppern | Medium](https://medium.com/@johannes.koeppern/self-consistency-with-chain-of-thought-cot-sc-2f7a1ea9f941#:~:text=,from%20all%20the%20answers%20found)). 가정은 “모델이 정답을 알고 있다면, 여러 번 시도했을 때 **대부분의 경로에서 정답을 내놓을 것**”이라는 것이며, 실제 실험에서 단일 추론 대비 답안 정확도가 크게 향상되었습니다 ([Self-Consistency with Chain of Thought (CoT-SC) | by Johannes Koeppern | Medium](https://medium.com/@johannes.koeppern/self-consistency-with-chain-of-thought-cot-sc-2f7a1ea9f941#:~:text=ADVANTAGEDESCRIPTIONQUANTIFICATIONImproved%20PerformanceSelf,N%2FA)). 이 방식은 각 추론 경로의 우연적 오류나 편향을 **투표를 통해 상쇄**하므로, 연쇄적 오류 전파를 일부 완화하는 효과가 있습니다. Google 등도 수학 문제 해결에서 이 기법을 활용하여 정확도를 높였다고 보고합니다. 이밖에 최근 연구들은 LLM 스스로 **출력의 확신도나 불확실성을 추정**하게 하여, 모델이 자신 없는 답을 할 때는 표시하거나 추가 검증을 거치게 하는 시도도 하고 있습니다. 예를 들어, **SAUP**(Situation Awareness Uncertainty Propagation) 등의 프레임워크는 LLM 에이전트의 각 추론 단계마다 **불확실성을 정량화**하여 오류 가능성을 추적함으로써, 위험 수준이 높아지면 다른 경로를 선택하는 등의 **방향 교정**을 할 수 있도록 합니다 ([A Complete Guide to Implementing Recursive/Multi-Step RAG | by Gaurav Nigam | aingineer | Medium](https://medium.com/aingineer/a-complete-guide-to-implementing-recursive-multi-step-rag-5afca90f57ee#:~:text=,inconsistencies%20during%20the%20recursive%20process)).

## Chain-of-Thought 한계와 개선 패턴

다단계 추론의 대표 격인 **Chain-of-Thought (CoT)** 기법은 프롬프트에 *“생각을 단계별로 설명해줘”*라고 요구하여 LLM이 **중간 추론 과정을 표현**하게 합니다. CoT는 모델의 **논리 전개**를 관찰하거나 토큰 생성을 유도하는 데 유용하지만, **내재 지식만으로 추론**하므로 잘못된 지식을 기반으로 **허위 사실을 만들어낼 우려**가 있습니다 ([ReAct Prompting | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/techniques/react#:~:text=tab%29%20arxiv,fact%20hallucination%20and%20error%20propagation)). 하나의 단계에서 잘못된 정보가 삽입되면 이후 모든 단계에 영향을 미쳐 최종 답변을 그릇되게 만들 수 있다는 것이 CoT의 한계로 지적됩니다 ([ReAct Prompting | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/techniques/react#:~:text=tab%29%20arxiv,fact%20hallucination%20and%20error%20propagation)). 예를 들어, CoT를 적용한 QA에서 모델이 **사실과 다른 단서**를 상상해버리면 이후 논리가 모두 무의미해지는 문제가 발생합니다.

이를 보완하기 위해 제안된 패턴 중 하나가 **ReAct**(Reason + Act)입니다 ([ReAct Prompting | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/techniques/react#:~:text=ReAct%20is%20a%20general%20paradigm,involved%20to%20perform%20question%20answering)). ReAct는 CoT의 추론(**Reasoning**) 단계와, 외부 지식에 접근하는 **행동(Action)** 단계를 **번갈아 수행**함으로써 모델이 **필요한 정보를 검색하거나 계산하며 추론**하도록 합니다 ([ReAct Prompting | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/techniques/react#:~:text=ReAct%20is%20a%20general%20paradigm,involved%20to%20perform%20question%20answering)). 예를 들어 HotpotQA처럼 장문 추론이 필요한 질의에 대해, 모델이 *“~를 찾을 필요가 있다”*는 **생각(Thought)**을 내면 곧바로 **위키피디아 검색(Action)**을 수행하고, 얻은 결과를 **관찰(Observation)**하여 다시 추론에 반영하는 식입니다 ([ReAct Prompting | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/techniques/react#:~:text=Image%3A%20REACT)). 이렇게 하면 모델이 모르는 사실은 **외부에서 가져와 확인**하게 되므로, CoT에서 흔했던 **환각이나 지식 누락 문제가 줄어듭니다 ([[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629#:~:text=trustworthiness%20over%20methods%20without%20reasoning,respectively%2C%20while%20being%20prompted%20with))**. 실제 Google 연구에 따르면, ReAct를 통해 위키 API와 상호작용하게 한 모델은 순수 CoT에 비해 **사실 오정보 및 오류 전파를 크게 줄이고** 결과의 해석 가능성과 신뢰도를 높였습니다 ([[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629#:~:text=trustworthiness%20over%20methods%20without%20reasoning,respectively%2C%20while%20being%20prompted%20with)). 아래 그림은 **ReAct 프롬프트 체계**의 예시로, Apple Remote에 대한 질문에 대해 모델이 **검색 행동**을 통해 정확한 단서를 찾아내고 최종적으로 사실 근거 있는 답을 도출하는 과정을 보여줍니다 ([ReAct Prompting | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/techniques/react)). 녹색으로 하이라이트된 부분처럼, CoT만으로 풀었다면 놓쳤을 **“Front Row (소프트웨어)”** 정보를 ReAct는 검색으로 확인하고 추론에 반영하여, 잘못된 초깃값(아이폰, 아이패드가 정답이라는 오추론)을 바로잡고 **정답인 키보드 기능 키**를 찾아낸 것을 볼 수 있습니다 ([ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io#:~:text=over%20methods%20without%20reasoning%20or,prompted%20with%20only%20one%20or)) ([ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io#:~:text=The%20reason,a%20interpretable%20and%20factual%20trajectory)). ReAct 이외에도 **Toolformer**, **MRKL** 등 LLM이 API나 계산 엔진 등의 **외부 도구를 호출**하여 필요한 작업을 수행하는 다양한 에이전트 구조들이 제안되어 왔는데, 공통적으로 *“모델 혼자 상상하지 말고 **검증 가능한 채널**을 거쳐라”*는 철학으로 오류 전파를 막고자 합니다.

또 다른 개선 방향은 **Tree-of-Thoughts**나 **Language Model Monte Carlo Tree Search**처럼 **여러 가능한 추론 경로를 가지치기하면서 탐색**하는 방법입니다. 이는 CoT의 단일 선형 추론을 확장하여, 중간에 중요한 의사결정마다 둘 이상의 대안을 가지로 분기시킨 후, **각 경로의 잠재적 유망도를 평가**하여 최적 경로를 선택하거나 부족하면 다시 확장하는 방식입니다. 이러한 **탐색형 알고리즘**은 한 경로에 초기 오류가 있어도 다른 경로를 통해 정답에 도달할 수 있게 해주므로, **복잡한 퍼즐이나 증명 문제** 등에서 성공률을 높입니다. 예컨대 특정 퍼즐 문제를 풀 때 한 접근법이 막히면 다른 가정을 시도해보듯, LLM도 스스로 다양한 가정을 분기해서 해보고 최종 정답에 수렴하도록 하는 것입니다. 최근 Microsoft 등에서 소개된 **Guidance** 프레임워크도 유사한 아이디어로, LLM의 출력에 제약 조건(예: 반드시 어떤 키워드를 포함해야 함)을 걸어 **제약 충족형 탐색**을 수행함으로써 불합리한 경로로 빠지는 것을 방지합니다.

마지막으로, **Reflexion**이나 **Self-Refine** 기법처럼 **한 번 완성된 답변을 다시 분석하여 개선**하는 패턴도 주목받고 있습니다. 이러한 자기반영(self-reflection) 에이전트는 **과거 시도에서 성공과 실패를 학습**하기 위해 **에피소드 메모리**를 활용합니다 ([[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366#:~:text=methods%20require%20extensive%20training%20samples,making%2C%20coding)). 예를 들어 Reflexion 프레임워크에서는 LLM이 임무를 수행한 뒤 **환경으로부터 피드백**(성공/실패)을 받고, 그에 대한 **언어적 성찰 내용을 메모리**에 남깁니다 ([[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366#:~:text=methods%20require%20extensive%20training%20samples,making%2C%20coding)). 다음 시도 시 이 메모리를 참고하여 **이전 오류를 피하는 방향으로 결정**함으로써, 별도의 모델 재훈련 없이도 점진적인 성능 향상을 보였습니다 ([[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366#:~:text=updating%20weights%2C%20but%20instead%20through,4)) ([[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366#:~:text=baseline%20agent%20across%20diverse%20tasks,into%20how%20they%20affect%20performance)). 이러한 접근은 실제 강화학습처럼 수많은 시도를 요구하지 않으면서 **언어 피드백만으로 행동 개선**이 가능하다는 점에서 효율적입니다. 요약하면, **오류를 한 번에 완벽히 피하려 하기보다, 발견 즉시 교정하고 여러 번 재도전할 수 있게 설계**함으로써 다단계 시스템 전체의 신뢰도를 높일 수 있습니다.

## 할루시네이션 문제와 사실성 개선 (빅테크 사례)

LLM의 **할루시네이션(hallucination)**이란, **모델 출력이 그럴듯하지만 사실 무근**인 정보를 만들어내는 현상입니다. 이러한 거짓 정보 생성은 앞서 언급한 단계별 오류 전파의 결과일 수도 있고, 또는 모델이 **훈련 데이터의 통계적 패턴만 학습**한 한계에서 기인하기도 합니다 ([Hallucinations: Why AI Makes Stuff Up, and What's Being Done About It - CNET](https://www.cnet.com/tech/hallucinations-why-ai-makes-stuff-up-and-whats-being-done-about-it/#:~:text=In%20other%20words%2C%20the%20model,Soatto%20said)) ([Hallucinations: Why AI Makes Stuff Up, and What's Being Done About It - CNET](https://www.cnet.com/tech/hallucinations-why-ai-makes-stuff-up-and-whats-being-done-about-it/#:~:text=,what%20they%20have%20seen%20before)). 빅테크 기업들은 LLM을 제품화하면서 **사용자에게 신뢰할 만한 정보 제공**을 최우선 과제로 삼고, 이를 위해 **모델 개발 전 과정에 걸쳐 종합적인 대책**을 강구하고 있습니다. 주요 접근방법들을 사례와 함께 살펴보면 다음과 같습니다.

### 1. **훈련 데이터 품질 관리 및 정제**

**“Garbage in, garbage out”**이라는 말처럼, 훈련 코퍼스에 잘못되거나 불분명한 데이터가 섞이면 모델이 허위 출력을 내놓을 확률도 높아집니다. 빅테크들은 대용량 데이터를 활용하면서도 **품질 필터링 파이프라인**을 두어 노이즈를 줄이고자 합니다. 예를 들어 Meta의 LLaMA 등은 Common Crawl 웹자료를 사용할 때 **문서 내 중복 제거, 특정 언어/도메인 선별, 혐오/음란 필터** 등을 거쳐 비교적 **정제된 하위 집합**만 학습에 사용했습니다. Google의 경우도 **크라우드소스 지식(위키백과)**이나 **출판된 서적 코퍼스** 등 **신뢰도 높은 출처의 비중**을 높이고, 자동화된 휴리스틱으로 광고/스팸 페이지를 걸러내는 등 노력을 기울였습니다.

또한 **학습 데이터에 출처 정보(meta-data) 부착**을 고민하는 흐름도 있습니다. 이는 모델이 특정 사실을 학습할 때 함께 저장된 출처를 기억하거나, 최소한 응답 생성 후 **출처를 다시 조회**하여 검증할 수 있게 하려는 것입니다. 예컨대 OpenAI는 모델 훈련 시 인터넷 자료를 광범위하게 사용하면서도, 나중에 ChatGPT가 그에 근거해 답변할 때 **출처 링크를 인용**하는 형태의 인터페이스(Bing 통합버전)를 도입했습니다. **DeepMind의 GopherCite** 시스템은 한발 더 나아가 아예 **답변 내에 근거가 된 웹 인용문을 포함**하도록 모델을 훈련했습니다 ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=Our%20latest%20work%20focuses%20on,the%20system%20is%20unable%20to)). GopherCite는 질문에 답할 때 관련 웹 검색을 수행하고, 자신이 찾아낸 **근거 문장을 따옴표와 함께 제시**함으로써 사용자가 **즉각적으로 사실 검증**을 할 수 있게 합니다 ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=Our%20latest%20work%20focuses%20on,the%20system%20is%20unable%20to)) ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=form%20an%20answer%20that%20can,of%20providing%20an%20unsubstantiated%20answer)). 그리고 충분한 근거를 찾지 못하면 억지로 답을 Hallucinate하지 않고 **“모르겠다”고 답하도록** 설계되었습니다 ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=Our%20latest%20work%20focuses%20on,the%20system%20is%20unable%20to)) ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=form%20an%20answer%20that%20can,of%20providing%20an%20unsubstantiated%20answer)). 이러한 **데이터 애트리뷰션(data attribution)** 기법은 모델의 기억에만 의존하지 않고 **외부 지식에 답을 근거짓는 습관**을 들인다는 점에서, 학습 단계에서부터 환각 감소에 효과적입니다.

**데이터 정제** 관점에서 놓치기 쉬운 부분은 **업데이트와 편향 제거**입니다. 세상 지식은 시시각각 변하기에, 오래된 데이터만으로 학습한 모델은 시점 차이로 엉뚱한 답을 할 수 있습니다. 이를 막기 위해 OpenAI, Anthropic 등은 모델을 정기적으로 **추가 파인튜닝**하거나 지식 그래프를 활용해 **업데이트된 사실을 주입**하는 방식을 연구하고 있습니다. 또한 데이터에 내재된 **편향(bias)**이나 **팩트체크 실패 사례**를 선별하여, 학습 시 가중치를 낮추거나 반례 데이터를 추가 제공하는 노력도 병행합니다 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=To%20achieve%20this%2C%20model%20deployers,essential%20to%20ensure%20balanced%20representation)). DataCamp의 분석에 따르면, **신뢰할 수 없는 출처를 배제하고 전문가가 검토한 콘텐츠를 포함**하는 등의 **엄격한 데이터 큐레이션**을 통해 모델의 환각 발생을 줄일 수 있다고 합니다 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=High,generate%20accurate%20and%20reliable%20responses)) ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=To%20achieve%20this%2C%20model%20deployers,essential%20to%20ensure%20balanced%20representation)). 요약하면, 빅테크들은 **대용량 데이터 수집 → 자동/수동 정제 → 지속적 업데이트**의 사이클을 구축하여 LLM의 지식 기반을 최대한 **사실에 가깝게 유지**하려는 노력을 기울이고 있습니다.

### 2. **모델 출력 신뢰도 평가 및 튜닝 (RLHF 등)**

훈련 단계의 노력만으로 환각을 완전히 없앨 수 없기에, **모델 출력물을 모니터링하고 추가 개선**하는 과정도 필수적입니다. 가장 대표적인 방법이 **인간 피드백 강화학습(RLHF)**입니다. OpenAI의 ChatGPT, Anthropic의 Claude, DeepMind의 Sparrow 모두 **사람 평가자**들을 활용해 모델 응답의 **사실성, 유용성, 해로움 여부** 등을 채점하고, 이를 보상 신호로 모델을 보정했습니다 ([Link](https://arxiv.org/pdf/2209.14375#:~:text=We%20present%20Sparrow%2C%20an%20information,to%20collect%20more%20targeted%20human)) ([Link](https://arxiv.org/pdf/2209.14375#:~:text=evidence%20from%20sources%20supporting%20factual,Finally%2C%20we%20conduct%20extensive)). 특히 Sparrow 사례에서, 인간 평가자가 *“근거를 잘 뒷받침하고 있는가?”*를 중점적으로 보도록 **규칙을 세분화**한 것이 특징인데, 그 결과 **78%의 답변에 신뢰할 만한 증거자료가 제시**되는 성과를 냈습니다 ([Link](https://arxiv.org/pdf/2209.14375#:~:text=evidence%20from%20sources%20supporting%20factual,Finally%2C%20we%20conduct%20extensive)). OpenAI도 InstructGPT 연구에서 사용자가 선호하는 응답을 학습시키는 과정에서 **“사실 여부”**를 평가 항목에 넣어, 거짓 정보를 생성할 때 감점되도록 했습니다. 반면 흥미롭게도 InstructGPT 논문에서는 **순전한 지도학습(SFT) 대비 RLHF 모델이 사실 오류 면에서는 더 나빠지는 경향**도 보고되었는데 ([RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html#:~:text=hallucination%20worse,model%20over%20SFT%20alone%20model)), 이는 사람들이 선호하는 스타일(자신만만하게 답변하기)을 학습하다 보니 **잘 모르는 것도 그럴듯하게 답하는 경향**이 생길 수 있음을 시사합니다. OpenAI 연구자들은 이를 해결하기 위해 **보다 정교한 보상모델**(예: 사실관계 오류시 크게 감점하는)이나 **모델 자신이 확실히 아는 내용만 답하도록 유도**하는 방법을 모색하고 있습니다 ([RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html#:~:text=hallucination%20by%20having%20a%20better,more%20for%20making%20things%20up)) ([RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html#:~:text=Image%3A%20RLHF%20makes%20hallucination%20worse,2022)). 요컨대 RLHF는 **모델의 답변 경향을 인간이 원하는 방향으로 조율**하는 강력한 수단이지만, **사람 평가자의 정확한 판단과 세심한 지침 설계**가 뒷받침되어야 제대로 효과를 볼 수 있습니다 ([RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html#:~:text=,proposed%20a%20couple%20of%20solutions)).

**자동화된 평가 및 사실 검증**도 중요한 보조 수단입니다. 예를 들어 Anthropic은 Claude 모델의 정밀도를 높이기 위해 **내부 평가 세트**를 만들어 모델이 잘 틀리는 유형의 질문들을 반복 테스트했습니다 ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=We%20tested%20Claude%202,rather%20than%20provide%20incorrect%20information)). Claude 2.1 발표에 따르면, 어려운 사실 질문들에 대해 **틀린 주장과 “잘 모르겠다”라는 답을 구분하는 척도**로 측정한 결과, 최신 모델이 이전보다 **거짓 정보 생성률을 절반 수준으로 낮추고** 대신 모르겠다고 넘기는 비율을 높였다고 합니다 ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=2x%20Decrease%20in%20Hallucination%20Rates)) ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=We%20tested%20Claude%202,rather%20than%20provide%20incorrect%20information)). 이는 모델이 **확실치 않은 영역에서는 함부로 단정하지 않도록** 조정되었음을 의미합니다. 한편, OpenAI는 앞서 소개한 CriticGPT처럼 **모델이 모델을 평가**하는 체계를 도입하기도 했습니다. GPT-4 수준의 모델에게 ChatGPT의 답과 원 질문을 모두 입력하여, **논리적 모순이나 사실 오류를 찾아내는 비평**을 출력하게 한 것입니다 ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=OpenAI%27s%20solution%20is%20to%20create,the%20initial%20prompt%20for%20context)). 이런 **2단계 출력 검증**을 거치면 인간 검수자의 부담을 덜면서도 실수를 더 많이 잡아낼 수 있다고 보고되고 있습니다 ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=In%20putting%20CriticGPT%20through%20its,flawless)). 이 외에도, 생성된 답변을 **검색 엔진으로 검증**하거나 **사전 구축된 지식베이스와 대조**하는 **사실 체커 모듈**을 붙이는 시도도 있습니다. Meta AI는 LLM 출력과 관련된 문장을 검색해줄 수 있는 **Attribution 알고리즘**을 개발하여, 모델이 자신있게 말한 내용이 실제 웹에 근거가 있는지 자동 확인하는 연구를 진행했습니다. 이러한 **사실 검증 단계**를 거치면, 최소한 명백한 환각(예: 존재하지 않는 인물이나 논문 언급 등)은 서비스 단계에서 걸러내거나 수정 요청을 재생성함으로써 최종 사용자에게 전달되지 않도록 할 수 있습니다.

### 3. **신뢰도 향상을 위한 시스템 아키텍처 (Bard, ChatGPT+Bing, Claude 등)**

궁극적으로 **사용자에게 직접 응답을 제공하는 애플리케이션 단계**에서, 각사는 다양한 **아키텍처적 안전장치**를 마련하고 있습니다. 현재 가장 널리 쓰이는 방법은 **Retrieval-Augmented Generation (RAG)**, 즉 **검색 및 정보검색 결합**입니다 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=Retrieval,date%20data)). Microsoft의 **빙 챗(Bing Chat)**은 GPT-4 기반 모델에 **웹 검색 기능**을 통합하여, 사용자의 질문에 대해 모델이 자체 지식만으로 답하지 않고 **실시간으로 관련 정보를 검색**하도록 합니다 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=By%20verifying%20AI,and%20prevent%20potentially%20harmful%20consequences)). 이 시스템은 **답변과 함께 출처 링크를 병기**하는데, PCMag 등의 리뷰에 따르면 Bing Chat이 **일관된 출처 인용으로 신뢰도를 높인다**는 평가를 받았습니다 ([Microsoft Bing Chat - Review 2023 - PCMag UK](https://uk.pcmag.com/ai/148145/microsoft-bing-chat#:~:text=Microsoft%20Bing%20Chat%20,with%20its%20consistent%20source%20citations)). OpenAI의 ChatGPT도 유사하게 웹 브라우징 플러그인을 통해 인터넷 자료를 읽고 답변에 활용할 수 있으며 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=By%20verifying%20AI,and%20prevent%20potentially%20harmful%20consequences)), 이를 통해 최신 정보나 모델 지식 너머의 사실들을 보다 정확히 전달하도록 합니다. Google의 **Bard** 역시 PaLM2 모델에 **Google 검색 및 지식그래프** 접근을 연결하여, 실제 검색 결과 근거로 답변하고 사용자가 **“Google it”** 버튼을 눌러 해당 근거를 확인할 수 있도록 설계되었습니다 (초기 버전에서는 인용 표기가 미흡해 지적을 받았으나 이후 개선 중입니다). 이렇듯 **LLM + 검색엔진 결합** 구조는 질문에 대한 **현실 세계의 근거자료**를 모델에게 제공함으로써, 환각을 줄이는 가장 직접적인 방법으로 인정받고 있습니다 ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=Our%20latest%20work%20focuses%20on,the%20system%20is%20unable%20to)).

또한 특정 분야에 특화된 **도구 통합**도 활용됩니다. 예를 들어 ChatGPT는 **Wolfram Alpha** 플러그인을 통해 수학, 물리 계산 문제에 대해 **정확한 계산 결과**를 받아와 답변함으로써 계산 실수를 없앴습니다. 코딩 질문의 경우 **Code Interpreter**나 **자동 컴파일 실행** 기능을 통해, 모델이 작성한 코드의 **오류 여부를 즉시 피드백**받아 잘못된 가정을 바로잡습니다. 이러한 **툴 사용 에이전트**들은 ReAct 패턴과 유사하게 *“행동하여 검증”*하기 때문에, 단순히 LLM 혼자 답변하는 것보다 훨씬 믿을 만한 출력을 제공합니다.

마지막으로 **Anthropic Claude**의 접근법은 **헌법 기반 AI(Constitutional AI)**로 불리는데, 이는 시스템 레벨에서 모델에게 **정확성과 정직성에 관한 원칙**을 주입하는 것입니다. Claude는 초기부터 *“도움이 되되, 정직하고, 해롭지 않을 것”*이라는 규칙 아래, 거짓 정보보다는 차라리 **모름을 인정**하도록 훈련되었습니다 ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=We%20tested%20Claude%202,rather%20than%20provide%20incorrect%20information)). 실제로 Claude 2.1 업그레이드에서 **부정확한 진술 발생률이 50% 감소**하고, 모호한 질문에 **“잘 모르겠다”라고 답하는 비율이 크게 늘어났다고** 밝혔는데 ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=2x%20Decrease%20in%20Hallucination%20Rates)) ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=We%20tested%20Claude%202,rather%20than%20provide%20incorrect%20information)), 이는 사용자에게 **확실한 정보만 제공**하려는 방향으로 모델을 조정한 결과입니다. 이러한 헌법식 접근은 RLHF와 더불어, 모델이 언제 답을 자제해야 하는지 학습시킴으로써 터무니없는 환각 답변을 줄이는 데 기여하고 있습니다. Anthropic은 추가로 Claude 2.1에 **툴 사용 기능**까지 넣어, 필요한 경우 외부 지식베이스나 계산기를 활용하도록 하였는데 ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=advancements%20in%20key%20capabilities%20for,for%20our%20customers%20across%20models)) ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=API%20Tool%20Use)), 이를 통해 신뢰도와 활용도를 동시에 높이고자 합니다.

以上 내용을 표로 요약하면 다음과 같습니다:

<table>
<tr><th>오류 전파 완화 기법</th><th>설명 및 사례</th><th>효과</th></tr>
<tr>
  <td><b>문제 분해 및 검증</b><br>(Decomposition & Validation)</td>
  <td>복잡한 문제를 하위 문제로 나누어 단계별 해결. 각 단계 출력에 대한 제약 조건이나 사실 여부를 검증. 예: 질의를 여러 부분으로 쪼개 각 부분에 답하고 마지막에 종합하기 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=To%20enhance%20output%20reliability%2C%20we,blog%20on%20prompt%20optimization%20techniques)).</td>
  <td>한 단계의 오류가 그대로 다음 단계로 넘어가지 않도록 차단. 국소적인 오류 수정이 가능해져 최종 정확도 향상.</td>
</tr>
<tr>
  <td><b>체인-오브-띵크</b><br>(Chain-of-Thought, CoT)</td>
  <td>모델이 중간 추론 과정을 글로 표현하도록 유도 ([ReAct Prompting | Prompt Engineering Guide<!-- --> ](https://www.promptingguide.ai/techniques/react#:~:text=tab%29%20arxiv,fact%20hallucination%20and%20error%20propagation)). 논리를 따라가기 쉬우나, 외부 정보 없이 내부 지식만으로 추론하므로 잘못된 가정을 하면 오류가 누적될 수 있음.</td>
  <td>모델의 **추론 과정을 투명하게 확인** 가능. 다만 사실 검증 단계가 없으면 환각 위험 존재.</td>
</tr>
<tr>
  <td><b>ReAct 및 툴 사용</b><br>(Reason+Act Agents)</td>
  <td>CoT에 **외부 도구 액세스**를 통합하여, 모델이 “생각”과 “행동”을 교대로 수행 ([ReAct Prompting | Prompt Engineering Guide<!-- --> ](https://www.promptingguide.ai/techniques/react#:~:text=ReAct%20is%20a%20general%20paradigm,involved%20to%20perform%20question%20answering)). 필요한 정보를 검색하거나 계산하며 그 결과(Observation)를 다시 추론에 반영 ([ReAct Prompting | Prompt Engineering Guide<!-- --> ](https://www.promptingguide.ai/techniques/react#:~:text=Image%3A%20REACT)). 예: ReAct로 위키 검색을 병행한 QA는 환각과 오류 감소 ([[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629#:~:text=trustworthiness%20over%20methods%20without%20reasoning,respectively%2C%20while%20being%20prompted%20with)).</td>
  <td>모델이 **근거를 확인**하면서 답변하므로 사실성 크게 개선. **지식 업데이트**도 실시간으로 가능해져 최신 정보 반영.</td>
</tr>
<tr>
  <td><b>자가 성찰 & 반복</b><br>(Self-Reflection & Iteration)</td>
  <td>한 번 생성한 답변을 모델이 스스로 검토하고 개선하는 **피드백 루프**를 도입 ([Reflection Agents](https://blog.langchain.dev/reflection-agents/#:~:text=Reflection%20is%20a%20prompting%20strategy,information%20such%20as%20tool%20observations)). 또는 여러 시도를 하며 실패 경험을 메모리해 다음 시도에 반영 (Reflexion) ([[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366#:~:text=methods%20require%20extensive%20training%20samples,making%2C%20coding)).</td>
  <td>**점진적 향상**: 초기에는 부정확해도 몇 차례 교정 과정을 거치며 최종 답변의 정확도 상승. 사람이 개입하지 않아도 **자동 교정**.</td>
</tr>
<tr>
  <td><b>다중 경로 탐색</b><br>(Self-Consistency, Tree-of-Thought)</td>
  <td>하나의 질문에 대해 모델이 여러 번 응답을 생성하고, 그 중 다수가 일치하는 답을 선택 ([Self-Consistency with Chain of Thought (CoT-SC) | by Johannes Koeppern | Medium](https://medium.com/@johannes.koeppern/self-consistency-with-chain-of-thought-cot-sc-2f7a1ea9f941#:~:text=Let%E2%80%99s%20talk%20about%20a%20prompting,the%20source%20proves%20with%20experiments)). 또는 중간 단계마다 여러 분기 탐색 후 가장 좋은 경로 채택.</td>
  <td>**우연한 오류 상쇄**: 한 번 실수한 경로도 다른 경로에서 성공하면 무마. 복잡한 문제에서 **성공 확률**을 높임.</td>
</tr>
</table>

<table>
<tr><th>환각(Hallucination) 저감 대책</th><th>설명 및 도입 사례</th><th>효과</th></tr>
<tr>
  <td><b>데이터 정제 및 출처관리</b><br>(Data Quality & Attribution)</td>
  <td>훈련 데이터에서 노이즈/편향 제거 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=High,generate%20accurate%20and%20reliable%20responses)). 신뢰 가능한 코퍼스 위주 구성 및 업데이트. 모델이 출력에 근거를 제시할 수 있도록 학습 (예: GopherCite의 인용 답변) ([GopherCite: Teaching language models to support answers with verified quotes - Google DeepMind](https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/#:~:text=Our%20latest%20work%20focuses%20on,the%20system%20is%20unable%20to)).</td>
  <td>모델 지식의 **사실 정확도 기반 향상**. 출처 제시로 **투명성** 확보 및 사용자의 독립적 검증 가능.</td>
</tr>
<tr>
  <td><b>인간 피드백 및 RLHF</b><br>(Human Feedback & RLHF)</td>
  <td>사람이 모델 출력의 진위 여부를 평가해 피드백으로 사용 ([](https://arxiv.org/pdf/2209.14375#:~:text=We%20present%20Sparrow%2C%20an%20information,to%20collect%20more%20targeted%20human)). 좋은 답에는 보상, 환각에는 패널티를 주어 미세조정. OpenAI, DeepMind, Anthropic 모두 적용.</td>
  <td>모델이 **사람이 원하는 방향**으로 응답 스타일 변경 (더 정확하고 겸손하게). 다만 사람 평가자의 품질에 의존하며, 세심히 하지 않으면 부작용 가능 ([RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html#:~:text=hallucination%20worse,model%20over%20SFT%20alone%20model)).</td>
</tr>
<tr>
  <td><b>자동 사실 검증</b><br>(Automated Fact-Checking)</td>
  <td>출력된 답변을 별도 모듈이 **검증**. 예: OpenAI CriticGPT (GPT-4 기반 비평)가 답변의 오류 지적 ([OpenAI Built an AI To Catch ChatGPT's Hallucinations -- Pure AI](https://pureai.com/Articles/2024/07/12/OpenAI-CriticGPT.aspx#:~:text=OpenAI%27s%20solution%20is%20to%20create,the%20initial%20prompt%20for%20context)), 또는 답변 내용을 검색해서 나온 정보와 대조 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=By%20verifying%20AI,and%20prevent%20potentially%20harmful%20consequences)).</td>
  <td>명백한 허위정보를 **사전에 걸러내거나 수정**. 복잡한 분야에서 **이중 체크**로 안전망 강화. 단, 검증 모델 자체의 오류 가능성은 주의.</td>
</tr>
<tr>
  <td><b>검색 결합 및 실시간 정보</b><br>(Retrieval-Augmented Generation)</td>
  <td>모델이 답하기 전에 **외부 지식베이스/웹검색**을 수행하여 최신이고 검증된 정보에 접근 ([AI Hallucinations: A Guide With Examples | DataCamp](https://www.datacamp.com/blog/ai-hallucination#:~:text=By%20verifying%20AI,and%20prevent%20potentially%20harmful%20consequences)). Bing Chat, Bard 등이 채택. 답변에 출처를 달아 투명성 부여.</td>
  <td>**환각 발생률 현저 감소** – 모델이 모르는 것은 찾아보므로 지어낼 필요 감소. 시시각각 변하는 정보에도 **정확히 대응** 가능.</td>
</tr>
<tr>
  <td><b>보수적 응답 정책</b><br>(Conservative Generation)</td>
  <td>모델이 아리송한 질문에는 **추측 대신 답변 유보** 또는 충분한 단서 요구. Anthropic Claude의 헌법 AI처럼 “모르겠다면 솔직히 모른다고 말하기” 원칙을 강화 ([Introducing Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1#:~:text=We%20tested%20Claude%202,rather%20than%20provide%20incorrect%20information)).</td>
  <td>사실과 다른 내용을 **확신 있게 말하는 빈도 감소**. 사용자 신뢰도 향상 (틀리기보다는 안 하는 편이 나음). 다만 너무 자제하면 유용성 저하 가능, 균형 필요.</td>
</tr>
</table>

以上과 같이, 멀티스텝 LLM 시스템에서의 오류 전파와 환각 문제에 대하여 **데이터-모델-시스템 각 단계별로 종합적인 대책**이 모색되고 있습니다. 최신 거대 언어 모델 기반 서비스들은 이러한 기법들을 적절히 조합하여, **뛰어난 능력과 높은 신뢰성**을 겸비한 응답을 제공하고자 노력합니다. 분야별 특화 도구와 인간 피드백을 접목한 **하이브리드 AI**가 현실화됨에 따라, 앞으로 LLM의 활용도는 더욱 높아지되 그만큼 정확성에 대한 기대치도 함께 높아질 것입니다. 기업들은 이를 위한 **지속적인 평가(evaluation)와 피드백 루프**를 운영하고 있으며, 연구자들도 **모델의 사실적 추론 능력 향상**이라는 도전과제를 다양한 각도에서 풀어나가고 있습니다. 결국 “Hallucination-free” AI에 완전히 도달하는 것이 쉽진 않겠지만, 위에 소개한 설계 패턴과 사례들은 그 목표를 향한 유의미한 진전을 보여주고 있습니다.