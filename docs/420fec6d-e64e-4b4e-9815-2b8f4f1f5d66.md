---
publish: true
uuid: 420fec6d-e64e-4b4e-9815-2b8f4f1f5d66
---

## TL;DR

- 머신러닝 모델 서빙에는 Triton, TorchServe, TensorFlow Serving, ONNX Runtime, KServe, FastAPI/Flask 등이 있음.
- 고성능 서빙에는 Triton, Kubernetes 환경에서는 KServe, 간단한 API 서빙에는 FastAPI/Flask가 적합함.
- PyTorch 전용 모델 서빙은 TorchServe, TensorFlow 모델은 TensorFlow Serving이 적합함.
- ONNX Runtime은 가벼운 환경에서 빠른 추론이 가능함.

## 머신러닝 모델 서빙 옵션 비교

### NVIDIA Triton Inference Server

- 다양한 프레임워크 지원 (TF, PyTorch, ONNX, TensorRT 등).
- Multi-GPU 및 Dynamic Batching 기능 제공.
- 모델 앙상블 및 모니터링 기능 내장.
- 설정이 복잡하고 무거운 환경에서는 과할 수 있음.

### TorchServe (PyTorch 전용)

- PyTorch 모델을 바로 서빙 가능.
- Multi-threading 및 A/B 테스트 지원.
- Custom handler를 통한 전처리/후처리 가능.
- PyTorch 전용이므로 다른 프레임워크 모델 변환 필요.

### TensorFlow Serving (TFS)

- TensorFlow/Keras 모델 서빙에 최적화됨.
- SavedModel 형식을 즉시 로드 가능.
- Batch inference 및 Model versioning 지원.
- TensorFlow 전용이므로 PyTorch 모델 변환 필요.

### ONNX Runtime (ORT)

- 다양한 프레임워크 모델을 ONNX 변환 후 서빙 가능.
- CPU, GPU 및 Edge Device에서도 실행 가능.
- 매우 가볍고 빠른 추론 속도 제공.
- ONNX 변환이 필요하여 추가 과정 필요.

### KServe (Kubernetes 기반 서빙)

- Kubernetes 환경에서 배포 및 확장에 최적화됨.
- 여러 서빙 엔진과 통합 가능 (TF Serving, Triton, ONNX 등).
- Auto-scaling 및 Canary rollout 지원.
- 설정이 복잡하며 Kubernetes 환경이 필요함.

### FastAPI / Flask 기반 Custom 서빙

- 간단한 REST API 서버로 모델 서빙 가능.
- 특정 요구사항에 맞춰 커스텀 가능.
- 가벼운 배포 가능하지만, 스케일링이 어려움.
- 배치 추론 및 모델 버전 관리 기능이 기본적으로 제공되지 않음.

## 선택 가이드

- **고성능 & 대규모 배포:** Triton, KServe
- **PyTorch 모델 서빙:** TorchServe
- **TensorFlow 모델 서빙:** TensorFlow Serving
- **가벼운 배포 & 빠른 추론:** ONNX Runtime
- **Kubernetes 환경 & Auto-scaling:** KServe
- **소규모 & 간단한 API 서빙:** FastAPI / Flask