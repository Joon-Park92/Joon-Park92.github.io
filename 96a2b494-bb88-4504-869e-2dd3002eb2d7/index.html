<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My personal digital garden"><meta name=author content=Sean><link href=https://Joon-Park92.github.io/96a2b494-bb88-4504-869e-2dd3002eb2d7/ rel=canonical><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.9"><title>96a2b494 bb88 4504 869e 2dd3002eb2d7 - Sean</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Noto+Sans+Korean:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Noto Sans Korean";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../css/timeago.css><link rel=stylesheet href=../stylesheets/extra.css><link rel=stylesheet href=../stylesheets/links.css><link rel=stylesheet href=https://unpkg.com/katex@0/dist/katex.min.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=blue-grey data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#investigating-openais-deep-research-implementation class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=Sean class="md-header__button md-logo" aria-label=Sean data-md-component=logo> <img src=../img/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Sean </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 96a2b494 bb88 4504 869e 2dd3002eb2d7 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=blue-grey data-md-color-accent=light-blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=blue-grey data-md-color-accent=light-blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/Joon-Park92/Joon-Park92.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Joon-Park92/Joon-Park92.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=Sean class="md-nav__button md-logo" aria-label=Sean data-md-component=logo> <img src=../img/logo.png alt=logo> </a> Sean </label> <div class=md-nav__source> <a href=https://github.com/Joon-Park92/Joon-Park92.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Joon-Park92/Joon-Park92.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../c0295d5c-ac9d-4a38-a890-a550d8508304/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Think </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Think </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../71af9498-4da5-490e-8ca0-41b0d11cc8d7/ class=md-nav__link> <span class=md-ellipsis> problem </span> </a> </li> <li class=md-nav__item> <a href=../2f6da2cc-73a4-4dbb-aa14-4c2280df6a59/ class=md-nav__link> <span class=md-ellipsis> innovation </span> </a> </li> <li class=md-nav__item> <a href=../7ba945a8-e583-4b99-bdd8-5c0f84118c54/ class=md-nav__link> <span class=md-ellipsis> growth </span> </a> </li> <li class=md-nav__item> <a href=../bd21b1dc-6064-48b1-8894-18693847e844/ class=md-nav__link> <span class=md-ellipsis> periodic-phenomenon </span> </a> </li> <li class=md-nav__item> <a href=../e0a0fbf1-1ed1-4b59-ab3f-335ee2fd5ef6/ class=md-nav__link> <span class=md-ellipsis> loneliness </span> </a> </li> <li class=md-nav__item> <a href=../a544e931-db9d-4f20-b003-6a89f2d4a8d0/ class=md-nav__link> <span class=md-ellipsis> pokemon </span> </a> </li> <li class=md-nav__item> <a href=../3bc1acea-5edc-4594-984f-b5f284906524/ class=md-nav__link> <span class=md-ellipsis> duolingo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../cc4e555b-4456-4259-b261-6adb46e70c21/ class=md-nav__link> <span class=md-ellipsis> working-backward </span> </a> </li> <li class=md-nav__item> <a href=../6d61fad8-7e87-41a2-a4a7-95a83d249614/ class=md-nav__link> <span class=md-ellipsis> thinking-in-bets </span> </a> </li> <li class=md-nav__item> <a href=../7a8d3ccf-7ae1-4723-bf7e-36ac47970f2b/ class=md-nav__link> <span class=md-ellipsis> 무진기행 </span> </a> </li> <li class=md-nav__item> <a href=../5542ecb9-6658-4096-9cdb-2ac15a54277a/ class=md-nav__link> <span class=md-ellipsis> the-death-of-ivan-ilyitch </span> </a> </li> <li class=md-nav__item> <a href=../6f914835-5ecf-432a-9315-71fcca494238/ class=md-nav__link> <span class=md-ellipsis> the-old-man-and-the-sea </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Papers </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Papers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../98a670de-3076-4f48-879f-94727bfc8807/ class=md-nav__link> <span class=md-ellipsis> alpha-geometry </span> </a> </li> <li class=md-nav__item> <a href=../19a96d58-26fc-4835-a667-6e21c909a0f1/ class=md-nav__link> <span class=md-ellipsis> two-step-ml-enable-optimized-nano-particle-synthesis </span> </a> </li> <li class=md-nav__item> <a href=../c48e831f-9d8d-4cdd-88f1-9106a9a2db8e/ class=md-nav__link> <span class=md-ellipsis> self-route-rag-lc-hybrid-approach </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_1> <label class=md-nav__link for=__nav_5_1 id=__nav_5_1_label tabindex=0> <span class=md-ellipsis> ai-agent </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_1_label aria-expanded=false> <label class=md-nav__title for=__nav_5_1> <span class="md-nav__icon md-icon"></span> ai-agent </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../0e8f9f02-0c8d-4764-9952-512a95698684/ class=md-nav__link> <span class=md-ellipsis> definition </span> </a> </li> <li class=md-nav__item> <a href=../55f9b832-4723-4ac6-aba1-c5222f275773/ class=md-nav__link> <span class=md-ellipsis> improving-hallucination </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class=md-ellipsis> ml-design-pattern </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> ml-design-pattern </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../f8570381-560e-438d-b9f6-5250d6056f36/ class=md-nav__link> <span class=md-ellipsis> transform </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> theory </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> theory </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../e1f6d3f6-8a10-4ea0-a3b8-1167b0c9691f/ class=md-nav__link> <span class=md-ellipsis> singular-value-decomposition </span> </a> </li> <li class=md-nav__item> <a href=../a76b9851-0a4b-498e-b949-012360173f1d/ class=md-nav__link> <span class=md-ellipsis> decision-theory </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class=md-ellipsis> models </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> models </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../3ccafe3b-c24d-4bbc-b8a5-487b3b83cd5a/ class=md-nav__link> <span class=md-ellipsis> gradient-boost </span> </a> </li> <li class=md-nav__item> <a href=../5608e722-eb92-471f-bd22-d563272ce76f/ class=md-nav__link> <span class=md-ellipsis> xgboost </span> </a> </li> <li class=md-nav__item> <a href=../430c4345-4509-464b-afa1-839643cb3e7f/ class=md-nav__link> <span class=md-ellipsis> multi-armed-bandit </span> </a> </li> <li class=md-nav__item> <a href=../4a8e19a2-737f-49e6-bfbc-43c4a15358f3/ class=md-nav__link> <span class=md-ellipsis> ctr-model </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4_5> <label class=md-nav__link for=__nav_5_4_5 id=__nav_5_4_5_label tabindex=0> <span class=md-ellipsis> object-detection </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4_5> <span class="md-nav__icon md-icon"></span> object-detection </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../cd9b93fb-9aeb-4c66-a285-8c2cfb580637/ class=md-nav__link> <span class=md-ellipsis> yolo-outline </span> </a> </li> <li class=md-nav__item> <a href=../180d0c57-3989-4bc7-a33e-36e11e6753a9/ class=md-nav__link> <span class=md-ellipsis> r-cnn-outline </span> </a> </li> <li class=md-nav__item> <a href=../662c2408-9a10-4d9c-9b2e-289909cd2181/ class=md-nav__link> <span class=md-ellipsis> r-cnn </span> </a> </li> <li class=md-nav__item> <a href=../87d3f8f6-0e00-4740-8fd7-5831847fbfb3/ class=md-nav__link> <span class=md-ellipsis> object-detection-metric </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class=md-ellipsis> metrics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> metrics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../755b070b-41b4-4ad1-896a-e89ed77bdea3/ class=md-nav__link> <span class=md-ellipsis> silhouette-score </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex=0> <span class=md-ellipsis> operations </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> operations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../420fec6d-e64e-4b4e-9815-2b8f4f1f5d66/ class=md-nav__link> <span class=md-ellipsis> serving-frameworks </span> </a> </li> <li class=md-nav__item> <a href=../97d4abaf-b54b-49e2-9eea-1262f3d60ffa/ class=md-nav__link> <span class=md-ellipsis> key-metrics-form-ml-api </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_7> <label class=md-nav__link for=__nav_5_7 id=__nav_5_7_label tabindex=0> <span class=md-ellipsis> data-pipeline </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> data-pipeline </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../8feb5ca2-1b5d-4e9b-aa95-c3cc19b3fada/ class=md-nav__link> <span class=md-ellipsis> data-pipeline-idempotent </span> </a> </li> <li class=md-nav__item> <a href=../544db2a0-4067-4464-b0d2-dbfcbd67f746/ class=md-nav__link> <span class=md-ellipsis> data-pipeline-best-practice </span> </a> </li> <li class=md-nav__item> <a href=../69c32762-4d5a-46af-980f-65412c50def8/ class=md-nav__link> <span class=md-ellipsis> data-pipeline-design-pattern </span> </a> </li> <li class=md-nav__item> <a href=../44a78a08-56b1-40e8-801f-5220573f7f63/ class=md-nav__link> <span class=md-ellipsis> data-pipeline-etl-elt-pattern </span> </a> </li> <li class=md-nav__item> <a href=../1e3e96b7-2d44-44c5-8987-b1f731d2322a/ class=md-nav__link> <span class=md-ellipsis> data-lake-warehouse-mart </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Data Analysis </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Data Analysis </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../33ddecc4-e433-47f3-9dbb-a0b82456d359/ class=md-nav__link> <span class=md-ellipsis> confounder-variable </span> </a> </li> <li class=md-nav__item> <a href=../8ea2fddb-bde7-4e71-bdac-b2f5b31dc35e/ class=md-nav__link> <span class=md-ellipsis> carrying-capacity </span> </a> </li> <li class=md-nav__item> <a href=../430a1ee4-5289-435c-97ea-96d4ab2c6fde/ class=md-nav__link> <span class=md-ellipsis> bayesian-problem-1 </span> </a> </li> <li class=md-nav__item> <a href=../909ed374-e61f-4332-9619-4dedc93439ba/ class=md-nav__link> <span class=md-ellipsis> bayesian-notation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Programming </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Programming </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_1> <label class=md-nav__link for=__nav_7_1 id=__nav_7_1_label tabindex=0> <span class=md-ellipsis> cheat-sheet </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_1> <span class="md-nav__icon md-icon"></span> cheat-sheet </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_1_1> <label class=md-nav__link for=__nav_7_1_1 id=__nav_7_1_1_label tabindex=0> <span class=md-ellipsis> vscode </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_1_1> <span class="md-nav__icon md-icon"></span> vscode </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../e526860c-b2ec-4be4-862e-06cb5188bd07/ class=md-nav__link> <span class=md-ellipsis> devcontainer </span> </a> </li> <li class=md-nav__item> <a href=../8e4310e9-3a6a-4d96-9256-e67659ca51cd/ class=md-nav__link> <span class=md-ellipsis> devcontainer-compose </span> </a> </li> <li class=md-nav__item> <a href=../46e14fc0-2be9-4b9f-a17a-e72ea38e6fe3/ class=md-nav__link> <span class=md-ellipsis> debug </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../0aed534e-ea29-417f-a8df-c3e52e03360d/ class=md-nav__link> <span class=md-ellipsis> golang </span> </a> </li> <li class=md-nav__item> <a href=../77b75667-9746-45c3-80a2-d8c45a40882a/ class=md-nav__link> <span class=md-ellipsis> pdb </span> </a> </li> <li class=md-nav__item> <a href=../953c51f3-f6f5-42d8-a87a-fb7f9493436d/ class=md-nav__link> <span class=md-ellipsis> dlv </span> </a> </li> <li class=md-nav__item> <a href=../3e95cbc6-7ecd-4b56-9ace-fa9df6f6d9b0/ class=md-nav__link> <span class=md-ellipsis> awk </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2> <label class=md-nav__link for=__nav_7_2 id=__nav_7_2_label tabindex=0> <span class=md-ellipsis> architecture-pattern </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_2_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2> <span class="md-nav__icon md-icon"></span> architecture-pattern </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../c4ae6334-c1c2-4452-890c-8a022d6c1246/ class=md-nav__link> <span class=md-ellipsis> uow-pattern </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_3> <label class=md-nav__link for=__nav_7_3 id=__nav_7_3_label tabindex=0> <span class=md-ellipsis> design-pattern </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_3_label aria-expanded=false> <label class=md-nav__title for=__nav_7_3> <span class="md-nav__icon md-icon"></span> design-pattern </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../abe10a12-9498-4435-816e-a967daeadd3f/ class=md-nav__link> <span class=md-ellipsis> diagram </span> </a> </li> <li class=md-nav__item> <a href=../5a69e196-3aa4-4c2c-858f-3b19507f6a9d/ class=md-nav__link> <span class=md-ellipsis> abstract-factory-pattern </span> </a> </li> <li class=md-nav__item> <a href=../5c2407d0-1b72-4886-9c6c-f73897c77475/ class=md-nav__link> <span class=md-ellipsis> builder-pattern </span> </a> </li> <li class=md-nav__item> <a href=../47efac8a-0a2b-4ca1-8154-022be7d39b79/ class=md-nav__link> <span class=md-ellipsis> singleton-pattern </span> </a> </li> <li class=md-nav__item> <a href=../883d7b99-7492-4055-9c0d-f742b6c6b94c/ class=md-nav__link> <span class=md-ellipsis> proxy-pattern </span> </a> </li> <li class=md-nav__item> <a href=../2b665dac-9091-4267-83de-36e9c3e15b6c/ class=md-nav__link> <span class=md-ellipsis> decorator-pattern </span> </a> </li> <li class=md-nav__item> <a href=../41d4ab75-8c96-4948-9616-db36d04b6b96/ class=md-nav__link> <span class=md-ellipsis> observer-pattern </span> </a> </li> <li class=md-nav__item> <a href=../743dc6d3-3c82-4714-852d-bb65861bbeb2/ class=md-nav__link> <span class=md-ellipsis> strategy-pattern </span> </a> </li> <li class=md-nav__item> <a href=../a74d9002-c7e9-4d06-b710-de589255cc97/ class=md-nav__link> <span class=md-ellipsis> memento-pattern </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_4> <label class=md-nav__link for=__nav_7_4 id=__nav_7_4_label tabindex=0> <span class=md-ellipsis> api-design </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_4_label aria-expanded=false> <label class=md-nav__title for=__nav_7_4> <span class="md-nav__icon md-icon"></span> api-design </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../70aaa37e-6bf4-4cda-9dab-756b32c2e6a8/ class=md-nav__link> <span class=md-ellipsis> resource-oriented-design </span> </a> </li> <li class=md-nav__item> <a href=../8ef7098d-dcc8-4211-9df2-adf369932457/ class=md-nav__link> <span class=md-ellipsis> aip-124 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_5> <label class=md-nav__link for=__nav_7_5 id=__nav_7_5_label tabindex=0> <span class=md-ellipsis> uml </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_5_label aria-expanded=false> <label class=md-nav__title for=__nav_7_5> <span class="md-nav__icon md-icon"></span> uml </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../1d7be764-7667-499d-8845-725d25979152/ class=md-nav__link> <span class=md-ellipsis> sequence-chart </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_6> <label class=md-nav__link for=__nav_7_6 id=__nav_7_6_label tabindex=0> <span class=md-ellipsis> test </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_6_label aria-expanded=false> <label class=md-nav__title for=__nav_7_6> <span class="md-nav__icon md-icon"></span> test </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ebc61d65-2267-48ec-b957-1915d16c3891/ class=md-nav__link> <span class=md-ellipsis> test-must-be-actionable </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_7> <label class=md-nav__link for=__nav_7_7 id=__nav_7_7_label tabindex=0> <span class=md-ellipsis> web </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7_7> <span class="md-nav__icon md-icon"></span> web </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dc73f07f-be90-45d6-a461-ba5bdcd5e163/ class=md-nav__link> <span class=md-ellipsis> auth-basic </span> </a> </li> <li class=md-nav__item> <a href=../d3c922e8-8eee-4d47-bfdb-54709ee13cf8/ class=md-nav__link> <span class=md-ellipsis> social-login </span> </a> </li> <li class=md-nav__item> <a href=../8477ba61-0609-4ac2-b988-7cbf8854c739/ class=md-nav__link> <span class=md-ellipsis> ca-certification </span> </a> </li> <li class=md-nav__item> <a href=../4ebc9c2a-c6c4-4427-8ff8-056a8ccbc38b/ class=md-nav__link> <span class=md-ellipsis> tcp-udp </span> </a> </li> <li class=md-nav__item> <a href=../3742ec8b-5f6a-46fc-b3c9-08ad198df8dc/ class=md-nav__link> <span class=md-ellipsis> osi-model </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_8> <label class=md-nav__link for=__nav_7_8 id=__nav_7_8_label tabindex=0> <span class=md-ellipsis> infra </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_8_label aria-expanded=false> <label class=md-nav__title for=__nav_7_8> <span class="md-nav__icon md-icon"></span> infra </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_8_1> <label class=md-nav__link for=__nav_7_8_1 id=__nav_7_8_1_label tabindex=0> <span class=md-ellipsis> database </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_8_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_8_1> <span class="md-nav__icon md-icon"></span> database </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../67718e56-966a-4592-8c6c-b5828d86dc56/ class=md-nav__link> <span class=md-ellipsis> transaction-and-lock </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_8_2> <label class=md-nav__link for=__nav_7_8_2 id=__nav_7_8_2_label tabindex=0> <span class=md-ellipsis> monitoring </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_8_2_label aria-expanded=false> <label class=md-nav__title for=__nav_7_8_2> <span class="md-nav__icon md-icon"></span> monitoring </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ca6f533e-9a0c-492c-9063-70bac671ae75/ class=md-nav__link> <span class=md-ellipsis> basic-loki-query </span> </a> </li> <li class=md-nav__item> <a href=../69a053ab-db6a-480f-8557-c127a2a6da79/ class=md-nav__link> <span class=md-ellipsis> basic-promql </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_8_3> <label class=md-nav__link for=__nav_7_8_3 id=__nav_7_8_3_label tabindex=0> <span class=md-ellipsis> k8s </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_8_3_label aria-expanded=false> <label class=md-nav__title for=__nav_7_8_3> <span class="md-nav__icon md-icon"></span> k8s </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00ec5c52-4cac-4577-a1e5-98d4fbde43dd/ class=md-nav__link> <span class=md-ellipsis> resource </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_8_4> <label class=md-nav__link for=__nav_7_8_4 id=__nav_7_8_4_label tabindex=0> <span class=md-ellipsis> terraform </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_8_4_label aria-expanded=false> <label class=md-nav__title for=__nav_7_8_4> <span class="md-nav__icon md-icon"></span> terraform </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../7d81fa63-d3fa-46c9-954d-f700780cdcf5/ class=md-nav__link> <span class=md-ellipsis> basic </span> </a> </li> <li class=md-nav__item> <a href=../9f796320-bd12-4bd9-ad27-420fecb00aa0/ class=md-nav__link> <span class=md-ellipsis> module </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex=0> <span class=md-ellipsis> Mathematics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Mathematics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../fe3a1ef1-ac09-47dd-8188-870c2b69e147/ class=md-nav__link> <span class=md-ellipsis> modern-mathematics </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8_2> <label class=md-nav__link for=__nav_8_2 id=__nav_8_2_label tabindex=0> <span class=md-ellipsis> linear-algebra </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_8_2_label aria-expanded=false> <label class=md-nav__title for=__nav_8_2> <span class="md-nav__icon md-icon"></span> linear-algebra </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../356067df-ccd5-4f2f-a732-f3e30fb096c4/ class=md-nav__link> <span class=md-ellipsis> laplace-fourier-transform </span> </a> </li> <li class=md-nav__item> <a href=../2ecc0325-cbba-4315-a434-a742533f59fd/ class=md-nav__link> <span class=md-ellipsis> jordan-normal-form </span> </a> </li> <li class=md-nav__item> <a href=../fc99afe0-532e-4a85-ba15-fdd5f7cd671f/ class=md-nav__link> <span class=md-ellipsis> spectral-theory </span> </a> </li> <li class=md-nav__item> <a href=../e1f6d3f6-8a10-4ea0-a3b8-1167b0c9691f/ class=md-nav__link> <span class=md-ellipsis> singular-value-decomposition </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8_3> <label class=md-nav__link for=__nav_8_3 id=__nav_8_3_label tabindex=0> <span class=md-ellipsis> abstract-algebra </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_8_3_label aria-expanded=false> <label class=md-nav__title for=__nav_8_3> <span class="md-nav__icon md-icon"></span> abstract-algebra </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../94229181-dc2c-4e33-ad8d-3e449257edbe/ class=md-nav__link> <span class=md-ellipsis> ideal </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8_4> <label class=md-nav__link for=__nav_8_4 id=__nav_8_4_label tabindex=0> <span class=md-ellipsis> analysis </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_8_4_label aria-expanded=false> <label class=md-nav__title for=__nav_8_4> <span class="md-nav__icon md-icon"></span> analysis </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../5457aa52-7690-4906-afc2-8243ab67a390/ class=md-nav__link> <span class=md-ellipsis> basic </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8_5> <label class=md-nav__link for=__nav_8_5 id=__nav_8_5_label tabindex=0> <span class=md-ellipsis> logic </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_8_5_label aria-expanded=false> <label class=md-nav__title for=__nav_8_5> <span class="md-nav__icon md-icon"></span> logic </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ecb6dfa9-eb48-4183-8e04-0b2d87f2fcea/ class=md-nav__link> <span class=md-ellipsis> completeness-soundness-consistency </span> </a> </li> <li class=md-nav__item> <a href=../1d76b428-9e2d-47ff-a222-33e64b28e53c/ class=md-nav__link> <span class=md-ellipsis> formal-system </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8_6> <label class=md-nav__link for=__nav_8_6 id=__nav_8_6_label tabindex=0> <span class=md-ellipsis> etc </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_8_6_label aria-expanded=false> <label class=md-nav__title for=__nav_8_6> <span class="md-nav__icon md-icon"></span> etc </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../b199b805-3a46-4d2e-9293-c78266395c1f/ class=md-nav__link> <span class=md-ellipsis> distance-vs-metric-vs-norm </span> </a> </li> <li class=md-nav__item> <a href=../5601ba05-e7ff-4dd2-a5fc-0be44f641911/ class=md-nav__link> <span class=md-ellipsis> tensor </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9> <label class=md-nav__link for=__nav_9 id=__nav_9_label tabindex=0> <span class=md-ellipsis> Economics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=false> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> Economics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../998de948-f260-4548-ac8e-d776cb700c5b/ class=md-nav__link> <span class=md-ellipsis> causal-relationship </span> </a> </li> <li class=md-nav__item> <a href=../7b126bd8-5469-4c24-bee7-782308dbf083/ class=md-nav__link> <span class=md-ellipsis> economic-agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10> <label class=md-nav__link for=__nav_10 id=__nav_10_label tabindex=0> <span class=md-ellipsis> Philosophy </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_10_label aria-expanded=false> <label class=md-nav__title for=__nav_10> <span class="md-nav__icon md-icon"></span> Philosophy </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../62bdf711-6616-41fe-a861-9b3b99a2e988/ class=md-nav__link> <span class=md-ellipsis> history </span> </a> </li> <li class=md-nav__item> <a href=../b173354e-9935-4dfe-ab04-77e62427478a/ class=md-nav__link> <span class=md-ellipsis> kant </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../newsletter/0_newsletter_index/ class=md-nav__link> <span class=md-ellipsis> Newsletters </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#investigating-openais-deep-research-implementation class=md-nav__link> <span class=md-ellipsis> Investigating OpenAI’s “Deep Research” Implementation </span> </a> <nav class=md-nav aria-label="Investigating OpenAI’s “Deep Research” Implementation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-system-architecture-and-technology-stack class=md-nav__link> <span class=md-ellipsis> 1. System Architecture and Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=#2-web-research-execution class=md-nav__link> <span class=md-ellipsis> 2. Web Research Execution </span> </a> </li> <li class=md-nav__item> <a href=#3-natural-language-query-understanding-and-response-generation class=md-nav__link> <span class=md-ellipsis> 3. Natural Language Query Understanding and Response Generation </span> </a> </li> <li class=md-nav__item> <a href=#4-user-request-routing-and-logic-handling class=md-nav__link> <span class=md-ellipsis> 4. User Request Routing and Logic Handling </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>96a2b494 bb88 4504 869e 2dd3002eb2d7</h1> <h2 id=investigating-openais-deep-research-implementation>Investigating OpenAI’s “Deep Research” Implementation<a class=headerlink href=#investigating-openais-deep-research-implementation title="Permanent link">⚑</a></h2> <h3 id=1-system-architecture-and-technology-stack>1. System Architecture and Technology Stack<a class=headerlink href=#1-system-architecture-and-technology-stack title="Permanent link">⚑</a></h3> <p><strong>Agent-Based Architecture:</strong> <em>Deep Research</em> is implemented as a specialized AI <strong>agent</strong> within the ChatGPT platform (<a href="https://www.infoq.com/news/2025/02/deep-research-openai/#:~:text=OpenAI%20has%20launched%20Deep%20Research%2C,analyzing%2C%20and%20synthesizing%20online%20information">OpenAI Launches Deep Research: Advancing AI-Assisted Investigation - InfoQ</a>). Unlike a single-turn chatbot response, this agent runs <strong>independently for an extended duration (5–30 minutes)</strong>, performing multi-step tasks autonomously (<a href="https://www.infoq.com/news/2025/02/deep-research-openai/#:~:text=and%20synthesizing%20online%20information">OpenAI Launches Deep Research: Advancing AI-Assisted Investigation - InfoQ</a>). At its core is a customized large language model (LLM) based on OpenAI’s GPT-4 architecture – referred to as the <strong>“o3” reasoning model</strong> – which has been optimized for long-form reasoning, web browsing, and data analysis (<a href="https://www.infoq.com/news/2025/02/deep-research-openai/#:~:text=and%20synthesizing%20online%20information">OpenAI Launches Deep Research: Advancing AI-Assisted Investigation - InfoQ</a>). This model is the brain of the system, orchestrating the research process and integrating with various tools.</p> <p><strong>Microservices and Tools:</strong> The Deep Research system is modular, with distinct components (microservices or tools) handling different functions. Key pieces likely include:</p> <ul> <li><strong>Web Search Service:</strong> Allows the agent to query the internet in real-time for relevant information. OpenAI provides a built-in web search tool (based on the same model used in ChatGPT’s search feature) to retrieve up-to-date results with citations (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=The%20Responses%20API%20comes%20with,assistant%20to%20locate%20previous%20cases">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>). This is presumably backed by a search API or engine (OpenAI hasn’t disclosed the provider, but it <em>“allows developers to get real-time information and citations from the web”</em> (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=The%20Responses%20API%20comes%20with,assistant%20to%20locate%20previous%20cases">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>)).</li> <li><strong>Browser/Content Fetcher:</strong> Enables the agent to <strong>navigate to web pages, retrieve their content (HTML, text, PDFs, images)</strong>, and pass that content back to the LLM for analysis (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Once%20it%20starts%20running%2C%20a,and%20more%2C%20then%20synthesizing%20results">Deep Research FAQ | OpenAI Help Center</a>). This component handles live crawling of pages returned by the search results. It likely includes parsers (for HTML, PDF, etc.) and possibly OCR or vision models for images so the LLM can interpret non-text data.</li> <li><strong>Python Execution Environment:</strong> A sandboxed code runner (similar to OpenAI’s Code Interpreter) that the agent can use for data analysis, computations, or generating charts/graphs (<a href="https://www.geeksforgeeks.org/chatgpt-deep-research-what-it-do-and-how-to-use-it/#:~:text=2.%20Real,LaTeX%2C%20Markdown%2C%20or%20interactive%20dashboards">OpenAI Launches Deep Research (New AI Feature for ChatGPT): What it Do and How to Use It - GeeksforGeeks</a>) (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=OpenAI%20said%20this%20version%20of,%E2%80%9D">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). The Deep Research model can decide to invoke this tool when needed – for example, to crunch numbers from a dataset or create a visualization. This environment is isolated for security and managed by OpenAI’s infrastructure (allowing only safe libraries and limited runtime).</li> <li><strong>Orchestration Layer:</strong> An internal orchestration system coordinates the LLM and tools. OpenAI’s recently announced <strong>Agents SDK</strong> is likely the framework underlying this coordination. The Agents SDK lets the developer (or system) define a set of available tools and then allows the AI agent to autonomously decide which tools to use, and in what sequence, to accomplish the goal (<a href="https://dev.to/bobbyhalljr/mastering-openais-new-agents-sdk-responses-api-part-1-2al8#:~:text=decisions,agent">Mastering OpenAI’s new Agents SDK &amp; Responses API [Part 1] - DEV Community</a>) (<a href="https://dev.to/bobbyhalljr/mastering-openais-new-agents-sdk-responses-api-part-1-2al8#:~:text=Unlike%20traditional%20chatbots%2C%20this%20AI,use%20based%20on%20user%20input">Mastering OpenAI’s new Agents SDK &amp; Responses API [Part 1] - DEV Community</a>). Each tool (search, browse, code, etc.) is exposed to the model via function calls. The <strong>Responses API</strong> serves as the interface for this, enabling the model to call functions dynamically and receive structured results (<a href="https://dev.to/bobbyhalljr/mastering-openais-new-agents-sdk-responses-api-part-1-2al8#:~:text=The%20Responses%20API%20is%20all,how%20AI%20interacts%20with%20users">Mastering OpenAI’s new Agents SDK &amp; Responses API [Part 1] - DEV Community</a>). In practice, the Deep Research agent’s LLM “thinks” about the user query, emits a function call (e.g. <code>web_search(query)</code>), the orchestrator executes that via the search service, then feeds the results back into the model’s context. This loop continues with the agent using tools and gathering information until it decides to stop and produce the final report.</li> <li><strong>Intermediate Storage &amp; Memory:</strong> During a 30-minute research session, the agent might accumulate a lot of data. The system likely maintains an ephemeral memory or storage for intermediate results – e.g. caching fetched web pages or storing summaries – so that the model can reference information discovered earlier without re-fetching it repeatedly. This could be an in-memory cache or a short-term database that holds the texts of visited pages, parsed data from files, etc. (OpenAI hasn’t detailed this, but such a store would be essential to handle “hundreds of sources” of information efficiently). The model itself has a large context window (GPT-4’s context can be tens of thousands of tokens) to hold many pieces of information at once, but for very long sessions the system might use summarization or compression of earlier findings to keep the context size manageable.</li> <li><strong>AI Model Integration Pipeline:</strong> The GPT-4 <em>“o3”</em> model is integrated via OpenAI’s model-serving stack (likely running on clusters of GPUs for inference). This model was <em>“trained through reinforcement learning on real-world tasks requiring browser and Python tool use”</em>, meaning the pipeline includes not just the base model, but also policy logic refined by feedback (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20beef%20up%20deep%20research%E2%80%99s,at%20the%20task%20going%20forward">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). The inference pipeline uses the function-calling capabilities of OpenAI’s Chat Completion API to let the model output tool actions in a structured format. The Agents SDK/Orchestrator monitors the model’s outputs; when a tool call is requested, it pauses the model, invokes the tool, and then resumes the model with the tool’s result injected into the prompt for the next step. This cycle is repeated iteratively. The entire process is managed as a job – when the user invokes Deep Research, a task is queued and the agent works asynchronously, streaming partial progress to the UI “Activity” sidebar and finally delivering the complete result.</li> </ul> <p>In terms of <strong>technology stack</strong>, OpenAI likely utilizes common cloud-native components: the services (search, browsing, Python sandbox) run on scalable infrastructure (Kubernetes or similar), and the LLM inference runs on optimized GPU servers. The Agents SDK itself is provided in Python (<a href="https://dev.to/bobbyhalljr/mastering-openais-new-agents-sdk-responses-api-part-1-2al8#:~:text=1">Mastering OpenAI’s new Agents SDK &amp; Responses API [Part 1] - DEV Community</a>) (<a href="https://dev.to/bobbyhalljr/mastering-openais-new-agents-sdk-responses-api-part-1-2al8#:~:text=from%20agent_sdk%20import%20Agent%2C%20WebSearchTool%2C,FileRetrievalTool">Mastering OpenAI’s new Agents SDK &amp; Responses API [Part 1] - DEV Community</a>), suggesting the orchestrator is written in Python and integrates with OpenAI’s API (for model calls and tools). The web browsing might leverage headless browser frameworks or custom HTTP clients. Overall, the architecture is a <strong>microservices + orchestrated AI model</strong> design: each tool is a microservice, and the LLM agent is the “brain” coordinating them via an orchestration layer (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=Along%20with%20the%20Responses%20API%2C,work%20toward%20a%20single%20goal">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>). This modular approach makes the system extensible – OpenAI noted that future versions will connect to <em>“more specialized data sources, including subscription-based and internal resources”</em> via this tool interface (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=Currently%2C%20ChatGPT%20deep%20research%E2%80%99s%20outputs,and%20internal%20resources%2C%20OpenAI%20added">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). In essence, Deep Research is built like a <strong>research assistant agent</strong>: it has access to a suite of services (search engine, browser, code executor, etc.) and a powerful reasoning model that drives the whole process.</p> <h3 id=2-web-research-execution>2. Web Research Execution<a class=headerlink href=#2-web-research-execution title="Permanent link">⚑</a></h3> <p><strong>Live Internet Searching:</strong> Deep Research performs live web research by issuing search queries and crawling online sources in real time. It <strong>does not rely solely on a pre-indexed corpus</strong>; instead, it actively queries the web when a task is run. According to OpenAI, the feature <em>“conducts multi-step research on the internet for complex tasks”</em> (<a href="https://openai.com/index/introducing-deep-research/#:~:text=Introducing%20deep%20research%20,It">Introducing deep research - OpenAI</a>). Under the hood, the agent uses a web search API/tool to retrieve current information. For example, it might query news sites, academic databases, or general search engines depending on the prompt. (OpenAI’s partnership with Microsoft suggests it may use Bing’s search API, though official sources just describe it generally as a <strong>web search tool with real-time info</strong> (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=The%20Responses%20API%20comes%20with,assistant%20to%20locate%20previous%20cases">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>).)</p> <p>Once the search results are obtained, the agent iteratively <strong>visits relevant links</strong>. It can click through multiple pages and even follow links within those pages if needed. As it <em>“brows[es] the web [and] interpret[s] content”</em> (<a href="https://www.infoq.com/news/2025/02/deep-research-openai/#:~:text=Unlike%20standard%20chatbot%20interactions%2C%20Deep,The%20tool">OpenAI Launches Deep Research: Advancing AI-Assisted Investigation - InfoQ</a>), the system parses the page text (and possibly metadata) and feeds those contents into the GPT-4o model for analysis. This is akin to a human opening search hits and reading them to gather facts. The agent’s design emphasizes finding <strong>authoritative sources</strong>: OpenAI has indicated that Deep Research tries to use <em>verified, reputable sources (e.g. scientific publications or official statistics)</em> for information (<a href="https://www.geeksforgeeks.org/chatgpt-deep-research-what-it-do-and-how-to-use-it/#:~:text=1.%20Cross,formats%20like%20LaTeX%2C%20Markdown%2C%20or">OpenAI Launches Deep Research (New AI Feature for ChatGPT): What it Do and How to Use It - GeeksforGeeks</a>). In practice, that means the agent’s search queries and link choices skew towards high-quality domains (for example, if researching a medical question, it might specifically seek pages on PubMed or WHO). The model likely learned during training to prefer sources that humans would consider trustworthy. However, it isn’t infallible – OpenAI cautions that the agent can sometimes <strong>fail to distinguish authoritative information from rumors or less reliable content</strong> (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=Still%2C%20OpenAI%20notes%20that%20ChatGPT,errors%20in%20reports%20and%20citations">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). To mitigate this, the agent doesn’t just grab the first answer it sees; it aggregates from multiple sources and provides citations so the user can verify claims (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=The%20big%20question%20is%2C%20just,%E2%80%9D">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>).</p> <p><strong>Source Evaluation and Selection:</strong> The Deep Research agent uses its LLM reasoning to evaluate search results and page content. It will typically perform multiple search queries throughout a session, refining them as it learns more about the topic. The decision of which link to click or which source to trust is part of the agent’s learned policy. In OpenAI’s earlier <em>WebGPT</em> research, the model was trained to quote from sources and was rewarded for citing trustworthy sites (<a href="https://arxiv.org/abs/2112.09332#:~:text=%3E%20Abstract%3AWe%20fine,a%20dataset%20of%20questions%20asked">[2112.09332] WebGPT: Browser-assisted question-answering with human feedback</a>). That research showed improved factual accuracy when the model had to back up answers with references. Deep Research inherits this philosophy: it actively <em>“collects references while browsing in support of [its] answers”</em> (<a href="https://arxiv.org/abs/2112.09332#:~:text=web,are%20preferred%20by%20humans%2056">[2112.09332] WebGPT: Browser-assisted question-answering with human feedback</a>). The agent likely looks at cues like domain reputation (e.g., <code>.edu</code>, <code>.gov</code> sites), recency of information (to get up-to-date data), and consistency across multiple sources. If three different reputable sources converge on the same fact, the agent gains confidence in that information. Conversely, if sources conflict, the agent might note the discrepancy or seek additional references. This behavior aligns with one of Deep Research’s stated capabilities: <em>“evaluating arguments, identifying biases, and suggesting counterpoints”</em> (<a href="https://www.geeksforgeeks.org/chatgpt-deep-research-what-it-do-and-how-to-use-it/#:~:text=2.%20Real,LaTeX%2C%20Markdown%2C%20or%20interactive%20dashboards">OpenAI Launches Deep Research (New AI Feature for ChatGPT): What it Do and How to Use It - GeeksforGeeks</a>) – essentially fact-checking and critical analysis. The model’s training via reinforcement learning would have included scenarios requiring it to decide which pieces of information to trust and include in the final report (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20beef%20up%20deep%20research%E2%80%99s,at%20the%20task%20going%20forward">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>).</p> <p><strong>Multi-Modal and Deep Content Handling:</strong> Beyond basic web pages, Deep Research can handle various content formats encountered during research. It can <em>“read through text, PDFs, images, and more”</em> as it gathers information (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Once%20it%20starts%20running%2C%20a,and%20more%2C%20then%20synthesizing%20results">Deep Research FAQ | OpenAI Help Center</a>). For PDFs, the system likely has a PDF-to-text converter (extracting text from the document for the LLM to ingest). If an image contains useful information (say, an infographic or a chart), the agent can utilize GPT-4’s vision analysis or an OCR tool to interpret it. OpenAI’s GPT-4 model has multi-modal capabilities (vision), so Deep Research might leverage that when it “sees” an image – for example, understanding a diagram or reading the text in a screenshot. All of this is orchestrated through tool use: an <strong>“image reading” function</strong> or <strong>document parser</strong> function would feed the processed content back to the model. By supporting these formats, the agent is not limited to HTML web articles; it can digest academic papers (PDFs), datasets or tables (via CSV/Excel and the Python tool), and even videos transcripts if needed (though video handling isn’t explicitly mentioned, text transcripts would be handled as any document).</p> <p><strong>Summarization and Synthesis:</strong> As the agent collects information from potentially <em>hundreds of pages</em>, it must condense and synthesize that data. The GPT-4o model performs on-the-fly summarization of sources to keep only the relevant points in focus. For instance, if one source provides a definition, another gives a statistic, and another an expert opinion, the model will distill each and integrate them into its working notes. The <strong>“Activity” sidebar in the UI shows a running summary of the model’s thought process and the websites visited</strong> (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Once%20it%20starts%20running%2C%20a,and%20more%2C%20then%20synthesizing%20results">Deep Research FAQ | OpenAI Help Center</a>) – this indicates that the agent is maintaining an internal summary as it goes. The thought summary might say, e.g., “<em>Searched for latest EV car models, reading review on CarAndDriver… gathering specs for top 3 electric SUVs</em>”. This not only keeps the user informed, but also serves as the model’s scratchpad to avoid forgetting what it’s found. The model uses these notes to decide next steps (what to search for next, which angle to explore) and ultimately as raw material for the final report.</p> <p><strong>Fact-Checking and Citations:</strong> Before presenting the final output, Deep Research cross-checks important facts and ensures each claim can be <strong>traced to a source</strong>. OpenAI explicitly requires that <em>“every ChatGPT deep research output will be fully documented, with clear citations… making it easy to reference and verify the information”</em> (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=The%20big%20question%20is%2C%20just,%E2%80%9D">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). In practice, the agent inserts footnote-style citations in the report, linking specific statements to the source URLs or documents. It can even <em>“cite specific sentences or passages from its sources”</em> when quoting or closely paraphrasing (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=massive%20amounts%20of%20text%2C%20images%2C,%E2%80%9D">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). This citation mechanism was likely baked into the prompt or learned during training (the model knows to output not just an answer, but append a reference marker for each fact). By design, this encourages the model to base its answers on found information (grounding the output) rather than purely on its parametric memory or imagination. It’s a safeguard against hallucination: if the model can’t find a source for something, it’s less likely to assert it as fact. Additionally, the presence of multiple citations allows a form of post-hoc fact-checking – the user can click the cited sources to verify correctness or read more detail. The system doesn’t perform a separate automated fact-check beyond what the agent itself does, but the <strong>multi-source synthesis inherently means the content has been vetted through several references</strong> during the research process.</p> <p>In summary, the web research execution involves <strong>live crawling of the internet via search queries</strong>, iterative filtering of relevant information, and thorough documentation. Deep Research essentially operates like a digital research analyst: it queries, reads, notes, cross-checks, and finally produces a well-sourced summary. OpenAI’s infrastructure ensures this happens within a safe sandbox – for example, the agent has <strong>read-only</strong> browsing (it can’t perform unauthorized actions on websites beyond fetching information), and any code it runs (via the Python tool) is in a controlled environment. The result is a balance between wide-ranging information access and controlled, reliable output. <em>Early evaluations suggest this approach improves accuracy</em>: on a challenging benchmark of expert-level questions (<em>Humanity’s Last Exam</em>), the Deep Research model scored twice as high as previous models, indicating its ability to understand context and retrieve the right info is superior (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=OpenAI%20said%20it%20tested%20ChatGPT,4o%20%283.3">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). Of course, if the web contains false or biased information, the agent might pick it up – hence the emphasis on user verification and critical review of the cited sources (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=Still%2C%20OpenAI%20notes%20that%20ChatGPT,errors%20in%20reports%20and%20citations">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). But overall, the system’s design (especially the citation requirement and use of multiple sources) is aimed at delivering <strong>well-substantiated answers</strong> rather than quick, unsupported replies.</p> <h3 id=3-natural-language-query-understanding-and-response-generation>3. Natural Language Query Understanding and Response Generation<a class=headerlink href=#3-natural-language-query-understanding-and-response-generation title="Permanent link">⚑</a></h3> <p><strong>Understanding User Queries:</strong> When a user submits a query to Deep Research, the system first interprets what exactly the user is asking for and what type of output is needed. This goes beyond basic language understanding – the agent tries to form a <strong>research plan</strong> from the query. The user might ask a broad question (“Compare the top electric cars in terms of safety and price”) or a very specific one (“Find any published clinical trial results for drug XYZ in the last 5 years”). In either case, the GPT-4o model will parse the request and internally break it down into sub-tasks. This is aligned with the <em>chain-of-thought (CoT)</em> reasoning approach, where instead of jumping directly to an answer, the model <strong>“generate[s] a sequence of intermediate reasoning steps – essentially, to ‘think out loud.’”</strong> (<a href="https://www.fromthenew.world/p/openai-deep-research-explains-itself#:~:text=AI%20is%20the%20concept%20of,latent%20knowledge%20when%20guided%20properly">OpenAI Deep Research Explains Itself - by Brian Chau</a>). By reasoning step-by-step, the model can figure out: What are the key pieces of this question? What information do I need first? Is the query ambiguous or does it require clarification? OpenAI has even built a mechanism for clarification: if the query is complex or underspecified, <em>Deep Research may generate a short form or follow-up questions to capture specific parameters before it starts</em> (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=In%20ChatGPT%2C%20select%20%E2%80%98Deep%20research%E2%80%99,more%20focused%20and%20relevant%20report">Deep Research FAQ | OpenAI Help Center</a>). For example, if a user asks for “the best intermediate snowboard,” the agent might prompt the user (via a form in the UI) to specify budget, height, skill level, etc., to tailor the research (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=In%20ChatGPT%2C%20select%20%E2%80%98Deep%20research%E2%80%99,more%20focused%20and%20relevant%20report">Deep Research FAQ | OpenAI Help Center</a>). This indicates that the system uses the LLM to not only answer but also to <strong>query the user for missing details</strong> – a form of interactive query understanding.</p> <p><strong>Task Decomposition:</strong> After clarifying the request, the agent decomposes the problem internally. Using the chain-of-thought technique, the model might outline steps such as: 1) search for background on the topic, 2) find data or specific facts required, 3) analyze or compare the data, 4) formulate conclusions/recommendations. This process happens in the model’s “mind” (its hidden scratchpad) and is guided by prompt instructions and the model’s training. In fact, OpenAI’s Deep Research agent was likely trained with a prompting paradigm similar to <strong>ReAct (Reason+Act)</strong> or other agentic frameworks, where the model’s output alternates between reasoning statements and actions (tool calls). The OpenAI Responses API makes this possible by allowing the model to emit a structured action (like <code>{"tool": "web_search", "input": "latest EV car safety ratings"}</code>) as part of its completion (<a href="https://dev.to/bobbyhalljr/mastering-openais-new-agents-sdk-responses-api-part-1-2al8#:~:text=The%20Responses%20API%20is%20all,how%20AI%20interacts%20with%20users">Mastering OpenAI’s new Agents SDK &amp; Responses API [Part 1] - DEV Community</a>). The orchestrator then executes that action and returns the result to the model, which continues its chain-of-thought with new information. This loop continues, so the model is effectively <strong>translating the user’s natural language question into a sequence of research steps.</strong> It might not explicitly output the plan to the user, but the <em>“Activity” sidebar shows a summary of its thinking and steps</em> (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Once%20it%20starts%20running%2C%20a,and%20more%2C%20then%20synthesizing%20results">Deep Research FAQ | OpenAI Help Center</a>), confirming that it is indeed following a multi-step game plan internally. Each iteration, the model reads the latest tool result and decides the next move (“Do I need another source? Should I run analysis on data I found? Have I gathered enough to answer the question?”).</p> <p><strong>LLM Orchestration &amp; Prompt Engineering:</strong> The Deep Research feature relies on heavy prompt engineering behind the scenes. There is a specialized <strong>system prompt (the “deep research global prompt”)</strong> that primes the model at the start of the session (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=How%20is%20deep%20research%20represented,in%20the%20Compliance%20API">Deep Research FAQ | OpenAI Help Center</a>). This prompt likely outlines the agent’s role (“You are ChatGPT’s Deep Research agent, an AI researcher that can use tools to find and verify information…”), the tools available (with instructions for how to format tool calls), and the requirements for the final output (e.g., “provide a comprehensive report with sources cited for each major claim, and include a summary of your reasoning”). This system prompt acts as the blueprint for the agent’s behavior. Additional prompt instructions might be injected during the process, for example: after each tool use, the orchestrator could add a brief summary of what was found into the prompt (so the model can keep a running memory), or a reminder of the task goals to keep it on track.</p> <p>The <strong>Agents SDK</strong> essentially automates much of this orchestration, so developers (and OpenAI internally) don’t have to hand-craft every prompt per step. Instead, the model was fine-tuned to follow a certain format for reasoning and tool use. One common pattern from research is to have the model output something like: “Thought: I need to find X. Action: web_search[‘query about X’].” The Responses API likely captures anything in the model’s output that looks like an action and executes it, then appends the result as observation: “Observation: (content from the web page)”. The model then continues: “Thought: From this content, it seems Y… Next, I should …” and so on. By structuring the interaction this way (often called <strong>thought-action-observation</strong> loop), the agent can dynamically respond to what it finds. OpenAI’s Olivier Godement described it as chaining <em>“atomic units”</em> of work (model + tool) to achieve complex tasks, which the Agents SDK manages (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=Along%20with%20the%20Responses%20API%2C,work%20toward%20a%20single%20goal">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>). This is a form of <strong>LLM orchestration</strong> where the agent’s long session is broken into many smaller LLM calls, each guided by the previous step’s outcome.</p> <p><strong>Reinforcement Learning and Model Tuning:</strong> A major reason the Deep Research agent can parse queries and execute on them so effectively is that it has been <strong>specially fine-tuned (and trained with reinforcement learning) for these research tasks</strong>. OpenAI took their base GPT-4-level model and further trained it on <em>“real-world tasks requiring browser and Python tool use,”</em> giving it feedback on how well it performed those tasks (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20beef%20up%20deep%20research%E2%80%99s,at%20the%20task%20going%20forward">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). Likely, the training involved simulating many research sessions: the model was asked a question, it generated a sequence of tool uses and a final answer, and it got rewarded for correct, well-sourced answers. Over time, this training teaches the model <em>how to decide on sub-tasks and when to invoke which tool</em>. For example, the model learns that if the question is about data or numbers, it should use the Python tool to compute or graph rather than trying to do math in its head. Or if the question is recent (“latest developments on X”), it learns to use the search tool to get current information. The outcome is an internal policy that is <strong>optimized for multi-step reasoning</strong>. In other words, the model not only understands natural language, but also has a kind of meta-cognitive ability to figure out <em>“what steps will lead me to the answer?”</em>. This makes a huge difference in complex queries: instead of the user guiding it step by step, the agent self-directs its research. The model’s architecture (GPT-4) was already well-suited for following instructions and reasoning, but this fine-tuning with tools and feedback supercharges its effectiveness at these tasks (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20beef%20up%20deep%20research%E2%80%99s,at%20the%20task%20going%20forward">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>).</p> <p><strong>Response Generation (Report Writing):</strong> After gathering sufficient information, the agent enters the final phase: composing the answer. Here, the LLM consolidates everything it has learned into a coherent <strong>natural language report</strong>. The style of the report is detailed and structured – often with an introduction, body sections (which may cover different facets of the query), and a conclusion or summary. Throughout the text, it interweaves the citations corresponding to facts or quotes. The generation is handled entirely by the LLM (no templates are strictly imposed, but the system prompt likely provides a general expected format). Since the agent has been tracking its “thought process” and key points from sources, it uses those as the basis for the write-up. The user’s original question and any follow-up specifications are also part of the context, to ensure the answer is on target. Notably, OpenAI has the model also produce <em>“a summary of its thinking”</em> as part of the output or alongside it (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=The%20big%20question%20is%2C%20just,%E2%80%9D">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). In the UI, the user can toggle an <em>“Activity” view that shows the research steps (sources visited, etc.)</em> and a <em>“Citations” view that lists all sources</em> (<a href=https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/ >OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). The final answer the user sees is the polished report in the chat thread, while these other views provide transparency.</p> <p>(<a href=https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/ >OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>) <em>Example of a Deep Research final report and the accompanying citations (shown on the right). In this case, the user asked for recommendations on intermediate freestyle snowboards, and the agent produced a detailed report (“Snowboards for Intermediate Freestyle Riders”) citing 15 different sources. The interface lets the user inspect which references back up each part of the answer, underscoring the system’s focus on verifiable information.</em></p> <p>Under the hood, generating the final answer might involve another prompt to the model like: “Now compile all the gathered information into a comprehensive answer. Be sure to cite sources for each claim. If applicable, include an overview of how you approached the problem.” The model then outputs the answer in a single long completion (streamed to the user once ready). Currently, outputs are <strong>text-only</strong>, but OpenAI plans to augment them with visuals: <em>“embedded images, data visualizations, and other ‘analytic’ outputs soon”</em> are on the roadmap (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=Currently%2C%20ChatGPT%20deep%20research%E2%80%99s%20outputs,and%20internal%20resources%2C%20OpenAI%20added">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). This means the agent will be able to include charts it generated or pertinent images from the web in-line with the text. The model is already capable of creating plots via the Python tool and can insert those (the system can handle images as part of the chat response). In future, if a question is, say, data-heavy (“Analyze sales trends and show a graph”), the Deep Research answer could contain a graph image along with the explanation.</p> <p><strong>Prompt Engineering &amp; Guardrails:</strong> The prompts used ensure the model remains on task and handles the query responsibly. For instance, the system likely includes instructions like “If you encounter conflicting information, note it and seek clarification” or “Do not include information that cannot be substantiated by sources.” The model also has safety filters from OpenAI’s side: queries that would lead to disallowed content are handled by the broader ChatGPT safety system. Moreover, because the agent can browse, OpenAI must have <em>filters on the browsing tool</em> (e.g., to avoid going to illicit sites or accessing private data). These aspects are part of the logic handling (some are implemented in the tools layer, some in the model’s prompt). On the model generation side, to avoid things like plagiarism, it’s instructed to either paraphrase or quote with attribution rather than just copying large text from sources. All these are prompt-engineered behaviors that OpenAI has tuned during development.</p> <p>In summary, the natural language query is <strong>translated into a series of actions by the LLM</strong>, using advanced prompting and a fine-tuned reasoning policy. The model effectively understands not just language but the <em>intent</em> and <em>requirements</em> behind the question, which allows it to produce a high-quality, targeted answer. By orchestrating multiple steps (search, read, analyze, write) it behaves much like a human expert researcher might. Techniques like chain-of-thought prompting significantly enhance its ability to tackle complex problems by breaking them down (<a href="https://www.fromthenew.world/p/openai-deep-research-explains-itself#:~:text=AI%20is%20the%20concept%20of,latent%20knowledge%20when%20guided%20properly">OpenAI Deep Research Explains Itself - by Brian Chau</a>), while the integration of tool use gives it the “hands” to act on its thoughts (searching the web, executing code, etc.). The final response is then generated in fluent, structured natural language by the same powerful model, ensuring the answer is not only factually grounded but also well-explained and contextually relevant.</p> <h3 id=4-user-request-routing-and-logic-handling>4. User Request Routing and Logic Handling<a class=headerlink href=#4-user-request-routing-and-logic-handling title="Permanent link">⚑</a></h3> <p><strong>Detecting When to Use Deep Research:</strong> OpenAI’s system distinguishes between a normal ChatGPT query and one that warrants the Deep Research agent primarily through <strong>explicit mode selection</strong> by the user. In the ChatGPT interface, the user must click the “Deep research” option before submitting their query (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=Image%3A%20deep%20research%20composerImage%20Credits%3AOpenAI">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). This toggles the back-end to route the request to the deep research pipeline instead of the standard GPT-4 model. Essentially, the user’s choice is the trigger – e.g., if you simply ask a question in the regular mode, ChatGPT will answer with a quick turn-around (possibly using a brief search if needed), but if you switch to Deep Research mode, the system knows to invoke the heavy-duty agent. This design gives users control over when to spend the extra time (and their monthly quota of deep-research queries) on a question. It aligns with OpenAI’s guidance: <em>“If you just need a short response or a casual conversation, deep research is likely not necessary… Deep research shines in complicated, multi-layered inquiries requiring data from across the web.”</em> (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=What%20if%20I%20only%20need,a%20quick%20answer">Deep Research FAQ | OpenAI Help Center</a>) (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Search%20is%20great%20for%20quick%2C,report%20with%20citations%20and%20data">Deep Research FAQ | OpenAI Help Center</a>). In other words, <strong>straightforward questions are handled by the standard LLM</strong>, whereas <strong>complex or open-ended questions can be escalated to the Deep Research agent</strong> by user intent.</p> <p>Behind the scenes, OpenAI could also implement some automatic logic to suggest Deep Research for certain queries. For example, if a Plus user asks, “Can you write a detailed market analysis on X with references,” the system might prompt the user: “This looks complex. Would you like to use Deep Research?” (This kind of suggestion isn’t confirmed in documentation, but it would be a sensible UX feature.) Regardless, as of now the routing is mostly manual via the UI selection. On the developer side, using the API, one could programmatically decide to route a query to a Deep Research agent. For instance, an application could have a classifier that detects that a user’s question is broad or requires current data and then call the Responses API with the deep-research agent enabled. OpenAI’s platform supports this by marking the request accordingly – the <strong>Compliance API</strong> even allows enterprise users to identify deep research queries by a flag or special prompt string (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=How%20is%20deep%20research%20represented,in%20the%20Compliance%20API">Deep Research FAQ | OpenAI Help Center</a>). This implies that under the hood, when Deep Research mode is engaged, the system includes a <em><code>"tool_name":"deep_research"</code></em> in the conversation metadata or prompt, which signals the backend to use the Deep Research agent and workflow (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=How%20is%20deep%20research%20represented,in%20the%20Compliance%20API">Deep Research FAQ | OpenAI Help Center</a>).</p> <p><strong>Simple vs Complex Query Handling:</strong> The difference between a normal query and a “deep research” query in practice comes down to depth and interactivity. A simple query (e.g., “What’s the capital of France?” or “Give me a quick summary of the latest iPhone features”) does not require multi-step reasoning; ChatGPT can answer it in one turn, perhaps with a quick search if it’s current. ChatGPT’s built-in <strong>Search</strong> feature (formerly “Browsing”) is optimized for these <em>“quick, interactive real-time answers”</em>, pulling a few web snippets and returning a brief summary with source links (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Deep%20research%20and%20search%20in,report%20with%20citations%20and%20data">Deep Research FAQ | OpenAI Help Center</a>). Deep Research, on the other hand, is intended for when the user’s request <strong>explicitly or implicitly demands comprehensive analysis</strong> – for example, comparing many options, compiling a report, or researching an unfamiliar topic in detail. In those cases, the user opts into a longer wait in exchange for a much more thorough answer. OpenAI describes the difference succinctly: <em>“Search is great for quick… answers… It pulls info from the web and gives a brief summary with links. Deep research… searches through hundreds of sources, analyzes the information, and puts together a detailed report with citations and data.”</em> (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Deep%20research%20and%20search%20in,report%20with%20citations%20and%20data">Deep Research FAQ | OpenAI Help Center</a>). So, the system differentiates by scope: <strong>few sources vs many sources, seconds of work vs minutes of work, short answer vs in-depth report</strong>.</p> <p><strong>Routing to the Appropriate Module:</strong> Once the user (or system logic) decides on deep research, the request is handed off to the Deep Research backend. Technically, this might involve routing the query to a different API endpoint or service dedicated to the agent. OpenAI likely maintains separate model endpoints for GPT-4o (the deep research model) and the standard GPT-4 model. The Orchestrator sees the flag and spawns a Deep Research agent session, whereas a normal query would just prompt the chat model directly. It’s akin to selecting a different “skill” for the AI. Notably, ChatGPT also has other agent modes (OpenAI also introduced an **“Operator” agent for executing tasks on a computer, for example) (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=%E2%80%9CThere%20are%20some%20agents%20that,%E2%80%9D">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>) (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=ChatGPT%20uses%20for%20search%20%2C,assistant%20to%20locate%20previous%20cases">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>). Each of these agent modes is invoked by a user choice and then routed to its specialized model+tool stack. This modular architecture ensures that using Deep Research is a conscious decision – it sandboxes the heavy agent so it doesn’t inadvertently run for every query.</p> <p>The system also incorporates <strong>usage limits and user tier logic</strong> as part of routing. Deep Research was first rolled out to Pro subscribers, with Plus/Team to follow, each with a fixed number of deep research queries allowed per month (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=OpenAI%20said%20it%E2%80%99s%20making%20deep,and%20the%20European%20Economic%20Area">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>) (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Who%20can%20use%20deep%20research,and%20what%20are%20the%20limits">Deep Research FAQ | OpenAI Help Center</a>). The backend checks the user’s plan and remaining quota when routing the request. If the user has exceeded their allowance, it might refuse or ask to wait until it resets. This mechanism indirectly encourages using Deep Research only when necessary (since it’s a limited resource), thereby filtering out trivial queries. A Plus user, for example, only gets 10 deep research runs per month initially (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Who%20can%20use%20deep%20research,and%20what%20are%20the%20limits">Deep Research FAQ | OpenAI Help Center</a>), so they’ll save it for truly complex tasks. In terms of logic handling, this means the system must manage a <strong>queue or scheduling</strong> for deep research jobs – since each can run up to 30 minutes, the system might not run them all concurrently if resources are limited. There may be an orchestration service that manages these long-running sessions, ensuring each gets the required model and tool time, and that results are returned to the correct user session upon completion (with a notification as noted in the UI: “you’ll get a notification when the research completes” (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20use%20ChatGPT%20deep%20research%2C,notification%20when%20the%20search%20completes">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>)).</p> <p><strong>Mixing Modes in a Conversation:</strong> Another aspect of routing is what happens <em>after</em> a deep research response is delivered. The Deep Research output appears in the chat like a message from ChatGPT. The user can follow up with further questions. At this point, the user might continue in normal mode (using the info from the report as context for GPT-4 to answer quickly) or trigger another deep research run. The system likely treats the deep research result as part of the chat history, so the next prompt could either be answered by the standard model (if the user doesn’t explicitly invoke deep research again) or by launching a new deep research sequence. The chat interface allows switching modes per query, not something that happens automatically mid-conversation. This is important for logic handling: the system doesn’t permanently switch the user to an agent – it’s a per-query decision. So routing is determined each time the user enters a query. The state (chat history) is preserved across modes, which suggests the backend can feed the previous conversation (including perhaps a summary of the deep research findings) into the prompt for a normal ChatGPT answer if needed. This seamless hand-off is an area of ongoing UX improvement. As of the latest info, mobile and desktop integrations of deep research are planned, indicating that the routing logic will extend to those clients as well, not just the web UI (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20use%20ChatGPT%20deep%20research%2C,notification%20when%20the%20search%20completes">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>).</p> <p><strong>Example of Routing Logic:</strong> Imagine a user asks: “Should I buy or lease a car? Provide a detailed analysis with recent data on cost differences.” If they are in normal mode, ChatGPT might give a generic but reasonably informed answer from its training knowledge. It might optionally use the Search feature to grab a statistic or two, but it won’t deeply scour sources. If the user specifically turns on Deep Research for this query, the system will route it to the agent. The agent will then likely search for latest car market trends, maybe pull data from financial sites or calculators, possibly run a quick computation (via Python) comparing long-term costs, and then produce a multi-page report with citations from say, consumer reports, financial blogs, etc. The difference in output quality and depth would be significant. Internally, the <strong>routing decision was simply the user clicking that “Deep Research” button</strong> – everything after follows the deep research code path.</p> <p>From a developer perspective, OpenAI’s design is to keep these flows separate to avoid confusion and performance issues. The <strong>standard ChatGPT (GPT-4)</strong> path is tuned for speed and conversational coherence, while <strong>Deep Research’s path is tuned for thoroughness and tool use</strong>. By explicitly routing queries, the system doesn’t have to dynamically decide “shall I do a deep dive?” for every input – that would add unnecessary overhead and potential errors. Instead, the user (or an application’s logic) makes that call. That said, the <em>capability</em> to decide could be built in: the model could have a trigger phrase or detection (“when the user asks for a comprehensive report, automatically switch modes”). OpenAI might explore more seamless integration in the future, but initial rollout keeps it user-driven.</p> <p>In conclusion, <strong>user request routing</strong> for Deep Research is a gated choice that directs the query to a specialized research agent instead of the standard chat responder. Simple queries go down the lightweight path (fast response, minimal tool usage), whereas complex research questions go down the heavyweight path (invoking the full web-mining, multi-step agent). This separation ensures efficiency and clarity – users get to choose depth vs. speed. OpenAI’s documentation emphasizes using the right tool for the job: <em>“For shorter, real-time conversations or simpler queries, you might prefer using GPT-4o (with or without Search), which responds almost instantly. Deep research, on the other hand, is built for more complex tasks that require greater depth and thoroughness.”</em> (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Deep%20research%20typically%20completes%20a,which%20responds%20almost%20instantly">Deep Research FAQ | OpenAI Help Center</a>) (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Deep%20research%20and%20search%20in,report%20with%20citations%20and%20data">Deep Research FAQ | OpenAI Help Center</a>). The system architecture supports this by maintaining distinct modules and pipelines for each, and by providing the interface and APIs to route queries appropriately. By prioritizing an explicit routing mechanism, OpenAI ensures that the powerful but resource-intensive Deep Research feature is applied only when it truly adds value – delivering exhaustive, source-backed answers for complex questions – while the everyday Q&amp;A is handled with the usual speed and conversational finesse of ChatGPT.</p> <p><strong>Sources:</strong> The implementation details above are drawn from OpenAI’s official announcements and documentation, as well as analyses by credible tech outlets and researchers. OpenAI’s announcement of Deep Research describes it as an agent leveraging the GPT-4 (o3) model with tool integration for web browsing and data analysis (<a href="https://www.infoq.com/news/2025/02/deep-research-openai/#:~:text=OpenAI%20has%20launched%20Deep%20Research%2C,analyzing%2C%20and%20synthesizing%20online%20information">OpenAI Launches Deep Research: Advancing AI-Assisted Investigation - InfoQ</a>) (<a href="https://www.infoq.com/news/2025/02/deep-research-openai/#:~:text=and%20synthesizing%20online%20information">OpenAI Launches Deep Research: Advancing AI-Assisted Investigation - InfoQ</a>). InfoQ and TechCrunch provided insight into the specialized model and its training (optimized for reasoning and equipped with browsing/Python tools) (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=To%20beef%20up%20deep%20research%E2%80%99s,at%20the%20task%20going%20forward">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>) (<a href="https://techcrunch.com/2025/02/02/openai-unveils-a-new-chatgpt-agent-for-deep-research/#:~:text=OpenAI%20said%20this%20version%20of,%E2%80%9D">OpenAI unveils a new ChatGPT agent for ‘deep research’ | TechCrunch</a>). The OpenAI Help Center FAQ offers guidance on when to use Deep Research vs. normal search, and how it operates with citations and a thinking log (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Once%20it%20starts%20running%2C%20a,and%20more%2C%20then%20synthesizing%20results">Deep Research FAQ | OpenAI Help Center</a>) (<a href="https://help.openai.com/en/articles/10500283-deep-research-faq#:~:text=Deep%20research%20and%20search%20in,report%20with%20citations%20and%20data">Deep Research FAQ | OpenAI Help Center</a>). Additionally, OpenAI’s release of the Agents SDK and Responses API gives a peek into the architecture of such agent systems, highlighting how tools and LLMs are orchestrated in a modular fashion (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=The%20Responses%20API%20comes%20with,assistant%20to%20locate%20previous%20cases">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>) (<a href="https://www.theverge.com/news/627556/openai-ai-agents-responses-api-agents-sdk#:~:text=Along%20with%20the%20Responses%20API%2C,work%20toward%20a%20single%20goal">OpenAI will let other apps deploy its computer-operating AI | The Verge</a>). Finally, earlier research like OpenAI’s WebGPT paper illuminates the benefits of a browser-equipped model collecting references to improve factual accuracy (<a href="https://arxiv.org/abs/2112.09332#:~:text=%3E%20Abstract%3AWe%20fine,a%20dataset%20of%20questions%20asked">[2112.09332] WebGPT: Browser-assisted question-answering with human feedback</a>) – principles clearly embodied in Deep Research’s design. Together, these sources paint a picture of Deep Research as a cutting-edge AI research assistant: one that combines a powerful language model with search, browsing, and analytical tools in a carefully structured pipeline to deliver trustworthy, in-depth answers.</p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="March 24, 2025 13:25:02"><span class=timeago datetime=2025-03-24T13:25:02+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="March 24, 2025 13:25:02">2025-03-24</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/lyz-code target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 480 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg> </a> <a href=https://lyz-code.github.io/blue-book/newsletter/0_newsletter_index target=_blank rel=noopener title=lyz-code.github.io class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["navigation.footer", "navigation.top", "content.code.annotate", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../js/timeago.min.js></script> <script src=../js/timeago_mkdocs_material.js></script> <script src=../assets/js/katex.js></script> <script src=https://unpkg.com/katex@0/dist/katex.min.js></script> <script src=https://unpkg.com/katex@0/dist/contrib/auto-render.min.js></script> </body> </html>